{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Greeting Waddup gang","title":"Home"},{"location":"#greeting","text":"Waddup gang","title":"Greeting"},{"location":"tools/calculator/","text":"Hi!","title":"Calculator"},{"location":"tutorials/build-your-local-ai/","text":"Build Your Local AI: From Zero to a Custom ChatGPT Interface with Ollama & Open WebUI Imagine having ChatGPT or DeepSeek-like capabilities right on your computer \u2014 no subscription fees, no privacy concerns, no waiting for responses, and complete customization options. Sounds too good to be true? It\u2019s not! Large Language Models (LLMs) have become indispensable tools for many of us. Whether you\u2019re using them to process text, learn new concepts, generate code solutions, automate workflows, or enjoy chatting with your computer buddy \u2014 they\u2019re changing how we work and create. Large Language Models (LLMs) have become indispensable tools for many of us. Whether you\u2019re using them to process text, learn new concepts, generate code solutions, automate workflows, or enjoy chatting with your computer buddy \u2014 they\u2019re changing how we work and create. But there\u2019s a catch with popular services like OpenAI, Anthropic, or Perplexity: they often come with limitations: Cost : Monthly subscriptions or per-token pricing can add up quickly Limited APIs : Restricted customization options Missing features : Many don\u2019t offer RAG (Retrieval-Augmented Generation) capabilities Privacy concerns : Your data might be used for training or stored on third-party servers Response delays : Peak usage times can mean long waits for responses The solution? A locally running LLM instance that you can fully customize to your specific needs \u2014 complete with a user-friendly interface that rivals commercial offerings. In this guide (about a 1-hour setup) , I\u2019ll walk you through setting up a robust local AI environment using free, open-source tools that provide: Total privacy (your data stays on your machine) No subscription costs Minimal response waiting time Nearly comparable results to high-end commercial LLMs Complete customizability for your specific use cases Personal note: I've been using this setup for two months, and it has cut my monthly AI subscription costs to zero while increasing my productivity. The initial setup time is worth every minute for the long-term benefits. Table of Contents Toolstack Overview Environment Setup Installing and Configuring Ollama Installing and Configuring Open WebUI Basic OpenWebUI Functionality Enabling Web Search Code Interpreter Creating a Basic Custom Model Conclusion & Next Steps Toolstack Overview Our local AI setup will use two primary tools: Ollama Ollama is an open-source framework designed specifically for running LLMs locally. It provides: Access to a wide variety of open-source models (DeepSeek, Llama, Phi, Mistral, Gemma, and many more) Text generation capabilities Multimodal support (for models that can process images) Efficient model management OpenWebUI OpenWebUI is currently the most prominent open-source project offering a UI interface for your Ollama instance. Think of it as your local version of the ChatGPT or Claude interface, but with even more features: User-friendly chat interface Model customization RAG capabilities Web search integration Code interpreter Complex workflow design And many more features are being actively developed The best part? OpenWebUI is constantly improving as passionate engineers contribute to this open-source project, bringing features from proprietary platforms to this free alternative. Requirements Before we start, make sure your system meets these minimum requirements: A GPU-powered laptop or desktop (minimum 4GB GPU memory, 8GB+ recommended) At least 20GB of free disk space (models can be large) A modern web browser Pro tip: While CPU-only setups technically work, they\u2019ll be significantly slower. Even a modest GPU will greatly improve your experience. Environment Setup Setting Up Python 3.11 Ollama and OpenWebUI work best with Python 3.11, so we\u2019ll start by making sure we have the correct version installed. First, check your current Python version: python --version If you don\u2019t have Python 3.11 , we\u2019ll install it using pyenv , an excellent tool for managing multiple Python versions: Note : Visit the pyenv GitHub repository for detailed installation instructions specific to your OS. For macOS users: 1. Install pyenv brew update brew install pyenv 2. Configure your shell Add these lines to your shell configuration file ( .bashrc , .zshrc , or equivalent): export PYENV_ROOT=\"$HOME/.pyenv\" export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init --path)\" eval \"$(pyenv init -)\" Then reload your shell: exec \"$SHELL\" 3. Install Python 3.11 pyenv install 3.11 4. Set it as your global Python version This step is optional but is recommended for quickly starting Ollama and Open WebUI from your terminal in future sessions. pyenv global 3.11 5. Verify the installation python --version You should see an output confirming Python 3.11.8 is now installed. Installing and Configuring Ollama Now that we have our Python environment ready let\u2019s install Ollama : 1. Download and Install Ollama Visit the Ollama GitHub page and follow the installation instructions for your OS. For macOS users , download the .zip package from the Ollama official repository, unzip the file and install it by clicking the \u2018Ollama\u2019 application file. 2. Verify Ollama Installation Open a terminal/command prompt and type: ollama You should see Ollama startup with a help message indicating it\u2019s running. 3. Exploring Available Models Ollama gives you access to many open-source models. You can browse available models at ollama.com/search . When selecting a model, consider: Model size (smaller models run faster but may be less capable) Specialization (some models excel at coding, others at creative writing) Memory requirements (larger models need more GPU memory) Check the HuggingFace Open LLM Leaderboard for benchmarks and performance metrics. Use the advanced filters and metrics to find suitable models for your task. Pro tip : Start with smaller models (7B parameters or less) and move to larger ones only if needed. Many tasks can be handled effectively by smaller models, which run much faster. 4. Running Your First Model Let\u2019s start with phi4-mini, a small but capable model: ollama run phi4-mini This will download the model (if it\u2019s not already downloaded) and start an interactive chat session. Try asking it a question to verify everything is working. To exit the chat, type /bye or press Ctrl+D. Speed check : For a smooth experience, a good rule of thumb is that your model should output at least 10\u201320 words per second. If it\u2019s much slower, you might try a smaller model or check if your GPU is properly utilized. Too slow!. Good enough! 5. Check Your Installed Models List all your installed models with: ollama list Installing and Configuring Open WebUI Now that Ollama is running, let\u2019s install Open WebUI to create a user-friendly interface. 1. Install Open WebUI The easiest way to install Open WebUI is using pip: pip install open-webui 2. Start OpenWebUI Make sure Ollama is running, then start Open WebUI with: open-webui serve TODO: add pic Note: Make sure that no other processes are using ports 8080 or 5173 3. Access the UI Open your web browser and navigate to: http://localhost:8080/ You should see the OpenWebUI interface welcoming you! 4. Interface Overview Take a moment to familiarize yourself with the interface: Chat Interface : The main area where you\u2019ll interact with your AI Models Menu : Select which model(s) to use Chat Controls : Configure system prompts and model parameters Settings : Access administrative features and customizations TODO: add pic Pro tip : You can even use multiple models in a chat simultaneously to compare responses and aggregate their knowledge. TODO: add pic Basic OpenWebUI Functionality Let\u2019s explore some of the powerful features of OpenWebUI. Enabling Web Search Web search allows your AI to access current information beyond its training data. OpenWebUI supports multiple search engines, giving you flexibility based on your needs. Here\u2019s how to set it up: Go to Settings \u2192 Admin Panel \u2192 Web Search Enable web search and select your preferred search engine TODO: add pic I\u2019ll focus on two popular options: Google PSE and Brave Search. Each has its advantages and disadvantages: Google PSE API Setup Advantages: King of search. Industry-leading search capabilities and relevance Generous free tier (10,000 requests per day) Comprehensive search results across the entire web Disadvantages: Privacy concerns (Google processes your queries \ud83d\ude22) More complex setup process Setup process: For detailed instructions on how to set up Google PSE, please refer to the Open WebUI documentation . Brave Search API Advantages : Privacy-focused (doesn\u2019t track your search queries) Independent search index (not relying on Google) Simple setup process Free tier available Disadvantages : Limited to 2,000 free queries per month Search results may sometimes be less comprehensive than Google Setup Process : Go to Brave Search API Sign up and verify your email Navigate to the \u201cSubscribe\u201d tab and choose the free subscription (requires card information) TODO: add pic Go to the \u201cAPI Keys\u201d tab and create a new API key > TODO: add pic Copy the token and paste it into Open WebUI\u2019s web search configuration TODO: add pic Test Search Functionality We can now test the search functionality by asking our AI agent for the weather forecast for this weekend. TODO: add pic Note : Only enable web search when you need recent information or are researching topics outside the model\u2019s knowledge, as it significantly slows down response time. For most general queries, the built-in knowledge of your model will be faster and sufficient. Code Interpreter Open WebUI\u2019s code interpreter transforms your AI assistant into a dynamic programming tool, enabling it to write and execute Python code directly within the chat interface. Why This Matters : Your AI can now solve problems with code, demonstrate processes, and allow you to modify solutions interactively. Key Features: Interactive Code Blocks : View and edit AI-generated code directly in your chat. Multiple Executions : Run code multiple times with different inputs. Real-time Feedback : Receive immediate results without switching platforms. No Execution Limits : Enjoy unrestricted code execution without arbitrary limits. When you request a task that benefits from computational assistance, the AI recognizes the need for code execution and generates appropriate Python code that helps it answer your query. Just make sure to enable the \u2018Code Interpreter\u2019 option in your chat. Enabling the Code Interpreter : Navigate to Settings \u2192 Admin Panel \u2192 Code Execution. Toggle the \u201cEnable Code Interpreter\u201d switch to On . TODO: add pic Example Use Case: Imagine you need to find the prime factorization of a large number. This task can be challenging for humans to perform quickly and accurately, and it can also be difficult for AI assistants. That\u2019s why we seek the assistance of a computer to help with this task. Proompt: what is the prime factors decomposition of 272894? Without Code Interpreter: TODO: add pic Headache-inducing, isn\u2019t it? With Code Interpreter: TODO: add pic ^_^ The code interpreter fundamentally changes how you interact with your AI, transforming it from a conversational assistant into a computational powerhouse that can directly solve problems and demonstrate solutions. This feature alone can justify the entire local setup process for many users, especially those with data analysis, programming, or mathematics. Creating a Basic Custom Model One of the most powerful features of Open WebUI is the ability to customize how your AI behaves. To unlock this feature: Go to Workspace \u2192 Models \u2192 Create New Select your model a name, and select a base model (e.g., phi4-mini) Customize its behaviour (e.g., by adding a system prompt guiding its behaviour) TODO: add pic For example, you could create a biology professor persona with this system prompt: You are a university professor specialising in Biology with a passion for frogs-and you have a charming lisp in your speech. When interacting with users: - Answer biology-related queries with clear, factual, and detailed explanations, mainly focusing on frog topics. - Explain complex concepts using analogies drawn from everyday scenarios, making them easier to grasp. - If a user's question is ambiguous or unclear, ask clarifying questions before providing a complete answer. - Regularly quiz the user on key points to confirm understanding. - Propose various follow-up questions or alternative learning directions to encourage further discussion. - Maintain a friendly, engaging, and scholarly tone, ensuring your unique lisp is reflected in your speech.' To get such wonderful AI assistant interactions: TODO : add pic Pro tip : Create different model configurations for different tasks \u2014 one for brainstorming, another for coding, and yet another for detailed explanations. Conclusion & Next Steps Congratulations! \ud83c\udf89 You now have a fully functional local AI environment that gives you: Privacy (your data stays on your machine) Cost savings (no subscription fees) Fast responses Customizable AI assistants In just about an hour, you\u2019ve set up an infrastructure that rivals commercial AI platforms, all while maintaining complete control over your data and experience. What We\u2019ve Accomplished \u2705 Set up the required Python environment \u2705 Installed and configured Ollama \u2705 Installed and set up OpenWebUI \u2705 Enabled web search capabilities \u2705 Activated the code interpreter \u2705 Created a basic custom model Next Steps: If you want to advance your project or learn more about Retrieval-Augmented Generation (RAG) and custom knowledge bases, check out the next article: \"Open WebUI Tutorial \u2014 Supercharge Your Local AI with RAG and Custom Knowledge Bases\". This guide walks you through the out-of-the-box RAG features in Open WebUI that require no coding. By the end of the tutorial, you\u2019ll be able to build your own local documentation assistant.","title":"Build Your Local AI. From Zero to a Custom ChatGPT Interface with Ollama & Open WebUI"},{"location":"tutorials/build-your-local-ai/#build-your-local-ai-from-zero-to-a-custom-chatgpt-interface-with-ollama-open-webui","text":"Imagine having ChatGPT or DeepSeek-like capabilities right on your computer \u2014 no subscription fees, no privacy concerns, no waiting for responses, and complete customization options. Sounds too good to be true? It\u2019s not! Large Language Models (LLMs) have become indispensable tools for many of us. Whether you\u2019re using them to process text, learn new concepts, generate code solutions, automate workflows, or enjoy chatting with your computer buddy \u2014 they\u2019re changing how we work and create. Large Language Models (LLMs) have become indispensable tools for many of us. Whether you\u2019re using them to process text, learn new concepts, generate code solutions, automate workflows, or enjoy chatting with your computer buddy \u2014 they\u2019re changing how we work and create. But there\u2019s a catch with popular services like OpenAI, Anthropic, or Perplexity: they often come with limitations: Cost : Monthly subscriptions or per-token pricing can add up quickly Limited APIs : Restricted customization options Missing features : Many don\u2019t offer RAG (Retrieval-Augmented Generation) capabilities Privacy concerns : Your data might be used for training or stored on third-party servers Response delays : Peak usage times can mean long waits for responses The solution? A locally running LLM instance that you can fully customize to your specific needs \u2014 complete with a user-friendly interface that rivals commercial offerings. In this guide (about a 1-hour setup) , I\u2019ll walk you through setting up a robust local AI environment using free, open-source tools that provide: Total privacy (your data stays on your machine) No subscription costs Minimal response waiting time Nearly comparable results to high-end commercial LLMs Complete customizability for your specific use cases Personal note: I've been using this setup for two months, and it has cut my monthly AI subscription costs to zero while increasing my productivity. The initial setup time is worth every minute for the long-term benefits.","title":"Build Your Local AI: From Zero to a Custom ChatGPT Interface with Ollama &amp; Open WebUI"},{"location":"tutorials/build-your-local-ai/#table-of-contents","text":"Toolstack Overview Environment Setup Installing and Configuring Ollama Installing and Configuring Open WebUI Basic OpenWebUI Functionality Enabling Web Search Code Interpreter Creating a Basic Custom Model Conclusion & Next Steps","title":"Table of Contents"},{"location":"tutorials/build-your-local-ai/#toolstack-overview","text":"Our local AI setup will use two primary tools:","title":"Toolstack Overview"},{"location":"tutorials/build-your-local-ai/#ollama","text":"Ollama is an open-source framework designed specifically for running LLMs locally. It provides: Access to a wide variety of open-source models (DeepSeek, Llama, Phi, Mistral, Gemma, and many more) Text generation capabilities Multimodal support (for models that can process images) Efficient model management","title":"Ollama"},{"location":"tutorials/build-your-local-ai/#openwebui","text":"OpenWebUI is currently the most prominent open-source project offering a UI interface for your Ollama instance. Think of it as your local version of the ChatGPT or Claude interface, but with even more features: User-friendly chat interface Model customization RAG capabilities Web search integration Code interpreter Complex workflow design And many more features are being actively developed The best part? OpenWebUI is constantly improving as passionate engineers contribute to this open-source project, bringing features from proprietary platforms to this free alternative.","title":"OpenWebUI"},{"location":"tutorials/build-your-local-ai/#requirements","text":"Before we start, make sure your system meets these minimum requirements: A GPU-powered laptop or desktop (minimum 4GB GPU memory, 8GB+ recommended) At least 20GB of free disk space (models can be large) A modern web browser Pro tip: While CPU-only setups technically work, they\u2019ll be significantly slower. Even a modest GPU will greatly improve your experience.","title":"Requirements"},{"location":"tutorials/build-your-local-ai/#environment-setup","text":"","title":"Environment Setup"},{"location":"tutorials/build-your-local-ai/#setting-up-python-311","text":"Ollama and OpenWebUI work best with Python 3.11, so we\u2019ll start by making sure we have the correct version installed. First, check your current Python version: python --version If you don\u2019t have Python 3.11 , we\u2019ll install it using pyenv , an excellent tool for managing multiple Python versions: Note : Visit the pyenv GitHub repository for detailed installation instructions specific to your OS.","title":"Setting Up Python 3.11"},{"location":"tutorials/build-your-local-ai/#for-macos-users","text":"1. Install pyenv brew update brew install pyenv 2. Configure your shell Add these lines to your shell configuration file ( .bashrc , .zshrc , or equivalent): export PYENV_ROOT=\"$HOME/.pyenv\" export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init --path)\" eval \"$(pyenv init -)\" Then reload your shell: exec \"$SHELL\" 3. Install Python 3.11 pyenv install 3.11 4. Set it as your global Python version This step is optional but is recommended for quickly starting Ollama and Open WebUI from your terminal in future sessions. pyenv global 3.11 5. Verify the installation python --version You should see an output confirming Python 3.11.8 is now installed.","title":"For macOS users:"},{"location":"tutorials/build-your-local-ai/#installing-and-configuring-ollama","text":"Now that we have our Python environment ready let\u2019s install Ollama : 1. Download and Install Ollama Visit the Ollama GitHub page and follow the installation instructions for your OS. For macOS users , download the .zip package from the Ollama official repository, unzip the file and install it by clicking the \u2018Ollama\u2019 application file. 2. Verify Ollama Installation Open a terminal/command prompt and type: ollama You should see Ollama startup with a help message indicating it\u2019s running. 3. Exploring Available Models Ollama gives you access to many open-source models. You can browse available models at ollama.com/search . When selecting a model, consider: Model size (smaller models run faster but may be less capable) Specialization (some models excel at coding, others at creative writing) Memory requirements (larger models need more GPU memory) Check the HuggingFace Open LLM Leaderboard for benchmarks and performance metrics. Use the advanced filters and metrics to find suitable models for your task. Pro tip : Start with smaller models (7B parameters or less) and move to larger ones only if needed. Many tasks can be handled effectively by smaller models, which run much faster. 4. Running Your First Model Let\u2019s start with phi4-mini, a small but capable model: ollama run phi4-mini This will download the model (if it\u2019s not already downloaded) and start an interactive chat session. Try asking it a question to verify everything is working. To exit the chat, type /bye or press Ctrl+D. Speed check : For a smooth experience, a good rule of thumb is that your model should output at least 10\u201320 words per second. If it\u2019s much slower, you might try a smaller model or check if your GPU is properly utilized.","title":"Installing and Configuring Ollama"},{"location":"tutorials/build-your-local-ai/#installing-and-configuring-open-webui","text":"Now that Ollama is running, let\u2019s install Open WebUI to create a user-friendly interface. 1. Install Open WebUI The easiest way to install Open WebUI is using pip: pip install open-webui 2. Start OpenWebUI Make sure Ollama is running, then start Open WebUI with: open-webui serve TODO: add pic Note: Make sure that no other processes are using ports 8080 or 5173 3. Access the UI Open your web browser and navigate to: http://localhost:8080/ You should see the OpenWebUI interface welcoming you! 4. Interface Overview Take a moment to familiarize yourself with the interface: Chat Interface : The main area where you\u2019ll interact with your AI Models Menu : Select which model(s) to use Chat Controls : Configure system prompts and model parameters Settings : Access administrative features and customizations TODO: add pic Pro tip : You can even use multiple models in a chat simultaneously to compare responses and aggregate their knowledge. TODO: add pic","title":"Installing and Configuring Open WebUI"},{"location":"tutorials/build-your-local-ai/#basic-openwebui-functionality","text":"Let\u2019s explore some of the powerful features of OpenWebUI.","title":"Basic OpenWebUI Functionality"},{"location":"tutorials/build-your-local-ai/#enabling-web-search","text":"Web search allows your AI to access current information beyond its training data. OpenWebUI supports multiple search engines, giving you flexibility based on your needs. Here\u2019s how to set it up: Go to Settings \u2192 Admin Panel \u2192 Web Search Enable web search and select your preferred search engine TODO: add pic I\u2019ll focus on two popular options: Google PSE and Brave Search. Each has its advantages and disadvantages:","title":"Enabling Web Search"},{"location":"tutorials/build-your-local-ai/#google-pse-api-setup","text":"Advantages: King of search. Industry-leading search capabilities and relevance Generous free tier (10,000 requests per day) Comprehensive search results across the entire web Disadvantages: Privacy concerns (Google processes your queries \ud83d\ude22) More complex setup process Setup process: For detailed instructions on how to set up Google PSE, please refer to the Open WebUI documentation .","title":"Google PSE API Setup"},{"location":"tutorials/build-your-local-ai/#brave-search-api","text":"Advantages : Privacy-focused (doesn\u2019t track your search queries) Independent search index (not relying on Google) Simple setup process Free tier available Disadvantages : Limited to 2,000 free queries per month Search results may sometimes be less comprehensive than Google Setup Process : Go to Brave Search API Sign up and verify your email Navigate to the \u201cSubscribe\u201d tab and choose the free subscription (requires card information) TODO: add pic Go to the \u201cAPI Keys\u201d tab and create a new API key > TODO: add pic Copy the token and paste it into Open WebUI\u2019s web search configuration TODO: add pic","title":"Brave Search API"},{"location":"tutorials/build-your-local-ai/#test-search-functionality","text":"We can now test the search functionality by asking our AI agent for the weather forecast for this weekend. TODO: add pic Note : Only enable web search when you need recent information or are researching topics outside the model\u2019s knowledge, as it significantly slows down response time. For most general queries, the built-in knowledge of your model will be faster and sufficient.","title":"Test Search Functionality"},{"location":"tutorials/build-your-local-ai/#code-interpreter","text":"Open WebUI\u2019s code interpreter transforms your AI assistant into a dynamic programming tool, enabling it to write and execute Python code directly within the chat interface. Why This Matters : Your AI can now solve problems with code, demonstrate processes, and allow you to modify solutions interactively. Key Features: Interactive Code Blocks : View and edit AI-generated code directly in your chat. Multiple Executions : Run code multiple times with different inputs. Real-time Feedback : Receive immediate results without switching platforms. No Execution Limits : Enjoy unrestricted code execution without arbitrary limits. When you request a task that benefits from computational assistance, the AI recognizes the need for code execution and generates appropriate Python code that helps it answer your query. Just make sure to enable the \u2018Code Interpreter\u2019 option in your chat. Enabling the Code Interpreter : Navigate to Settings \u2192 Admin Panel \u2192 Code Execution. Toggle the \u201cEnable Code Interpreter\u201d switch to On . TODO: add pic Example Use Case: Imagine you need to find the prime factorization of a large number. This task can be challenging for humans to perform quickly and accurately, and it can also be difficult for AI assistants. That\u2019s why we seek the assistance of a computer to help with this task. Proompt: what is the prime factors decomposition of 272894? Without Code Interpreter: TODO: add pic Headache-inducing, isn\u2019t it? With Code Interpreter: TODO: add pic ^_^ The code interpreter fundamentally changes how you interact with your AI, transforming it from a conversational assistant into a computational powerhouse that can directly solve problems and demonstrate solutions. This feature alone can justify the entire local setup process for many users, especially those with data analysis, programming, or mathematics.","title":"Code Interpreter"},{"location":"tutorials/build-your-local-ai/#creating-a-basic-custom-model","text":"One of the most powerful features of Open WebUI is the ability to customize how your AI behaves. To unlock this feature: Go to Workspace \u2192 Models \u2192 Create New Select your model a name, and select a base model (e.g., phi4-mini) Customize its behaviour (e.g., by adding a system prompt guiding its behaviour) TODO: add pic For example, you could create a biology professor persona with this system prompt: You are a university professor specialising in Biology with a passion for frogs-and you have a charming lisp in your speech. When interacting with users: - Answer biology-related queries with clear, factual, and detailed explanations, mainly focusing on frog topics. - Explain complex concepts using analogies drawn from everyday scenarios, making them easier to grasp. - If a user's question is ambiguous or unclear, ask clarifying questions before providing a complete answer. - Regularly quiz the user on key points to confirm understanding. - Propose various follow-up questions or alternative learning directions to encourage further discussion. - Maintain a friendly, engaging, and scholarly tone, ensuring your unique lisp is reflected in your speech.' To get such wonderful AI assistant interactions: TODO : add pic Pro tip : Create different model configurations for different tasks \u2014 one for brainstorming, another for coding, and yet another for detailed explanations.","title":"Creating a Basic Custom Model"},{"location":"tutorials/build-your-local-ai/#conclusion-next-steps","text":"Congratulations! \ud83c\udf89 You now have a fully functional local AI environment that gives you: Privacy (your data stays on your machine) Cost savings (no subscription fees) Fast responses Customizable AI assistants In just about an hour, you\u2019ve set up an infrastructure that rivals commercial AI platforms, all while maintaining complete control over your data and experience.","title":"Conclusion &amp; Next Steps"},{"location":"tutorials/build-your-local-ai/#what-weve-accomplished","text":"\u2705 Set up the required Python environment \u2705 Installed and configured Ollama \u2705 Installed and set up OpenWebUI \u2705 Enabled web search capabilities \u2705 Activated the code interpreter \u2705 Created a basic custom model","title":"What We\u2019ve Accomplished"},{"location":"tutorials/build-your-local-ai/#next-steps","text":"If you want to advance your project or learn more about Retrieval-Augmented Generation (RAG) and custom knowledge bases, check out the next article: \"Open WebUI Tutorial \u2014 Supercharge Your Local AI with RAG and Custom Knowledge Bases\". This guide walks you through the out-of-the-box RAG features in Open WebUI that require no coding. By the end of the tutorial, you\u2019ll be able to build your own local documentation assistant.","title":"Next Steps:"},{"location":"tutorials/effective-software-testing/","text":"Effective Software Testing: A Developer's Guide Effective and systematic software testing is critical for ensuring the reliability and quality of software systems. This guide provides a comprehensive overview of best code testing practices, inspired by the book 'Effective Software Testing: A Developer's Guide' by the software quality and testing expert Maur\u00edcio Aniche \ud83d\udc10. The goal is to help developers understand the importance of proper testing, how to approach it systematically, and how to implement it effectively in their projects. Why Testing is Important Quality Assurance : Testing is essential because software failures can have severe consequences for businesses and end-users. Developers bear responsibility for the quality of the software they produce. Bug Prevention : While simplicity in code design can reduce bugs, it cannot eliminate them. Testing is necessary to catch errors that arise from complex interactions within the software. Cost-Efficiency : The cost of fixing bugs in production is often higher than the cost of thorough testing during development. Systematic testing reduces the risk of releasing buggy software, saving time and resources in the long run. Code Improvement : Testing often leads to improvements in code quality. As developers write tests, they frequently identify areas where the code can be refactored for better clarity, efficiency, or maintainability. This process of writing tests and refactoring code results in more readable, maintainable, and robust software. Confidence in Changes : A comprehensive test suite gives developers confidence to make changes or add new features, knowing that existing functionality can be quickly verified. The Testing Pyramid Unit Testing Focus : Tests individual units (e.g., methods or functions) in isolation Benefits : Fast, easy to write, and control Limitations : May not catch bugs that arise from interactions between units Integration Testing Focus : Tests interactions between different units or with external systems Benefits : Identifies issues in the integration layer Limitations : More complex and time-consuming to write System Testing Focus : Tests the entire system as a whole, including all components and their interactions Benefits : Provides a realistic view of how the system behaves in production Limitations : Slower to execute, more prone to flaky results, and harder to write When to Use Each Unit Tests : For all business rules and logic Integration Tests : For complex integrations with external services System Tests : For critical workflows and high-risk areas of the application Specification-Based Testing Specification-based testing derives test cases from the software's requirements, ensuring the code meets its intended functionality. Steps for Specification-Based Testing Understand the Requirements : Identify what the software should and should not do based on requirements or user stories Explore the Program : Investigate how the software handles various inputs and outputs Identify Partitions : Group inputs into partitions where the software is expected to behave similarly Identify Boundaries : Focus on edge cases, such as values around decision points (e.g., >, <, == operators) Devise Test Cases : Create tests based on identified partitions and boundaries, prioritizing edge cases and unique combinations Automate Test Cases : Implement tests using tools like JUnit, ensuring tests are easy to read and maintain Augment with Creativity : Use experience and intuition to identify additional test cases that may not be directly derived from the specifications Example Walkthrough: User Registration Function Consider a user registration function with a username and password: Requirements : The username must be 3-20 characters long and contain only alphanumeric characters. The password must be at least 8 characters long and contain at least one number, one capital letter, and one symbol. Partitions : Username: null, too short, too long, invalid, valid Password: null, too short, invalid, valid Boundaries : The test cases will include usernames of different lengths, from the shortest (2 characters\u2014invalid, 3 characters\u2014valid) to the longest (20 characters\u2014valid, 21 characters\u2014invalid). The same applies to password length: 7 characters\u2014invalid, 8 characters\u2014valid. Test Cases : Test an empty username Test an empty password Test a valid username that is too short Test an invalid username of 3 and 20 characters Test a valid username Test a valid username that is too short Test a valid password that is too short Test an invalid password of 8 characters Test a valid password Augment with Experience : We might know from experience that users may attempt to use symbols in their passwords that could pose security risks (e.g., backticks (`), pipe (|), angle brackets (<, >), etc.). Test cases should be added to ensure that passwords containing such characters are deemed invalid. Structural Testing and Code Coverage Structural testing, also known as white-box testing, involves testing the internal structure of the code to ensure thorough coverage of all its components. Unlike specification-based (black-box) testing, which focuses on functional requirements, structural testing delves into the code to ensure that all paths, branches, and conditions are adequately tested. Steps for Structural Testing Perform Specification-Based Testing : Begin with tests derived from functional requirements to validate expected behaviour Review Code : Carefully read the unit implementation to understand the developer's key coding decisions Run Tests with Coverage Tools : Execute the devised test suite using code coverage tools Analyze Coverage Gaps : Identify any untested code segments revealed by the coverage analysis Decide on Testing Missed Code : Evaluate whether the untested code is significant enough to warrant additional testing Iterate and Refine : Revisit the source code to identify other test cases that could be derived from the code structure Code Coverage Criteria Line Coverage : Ensures that every line of code is executed at least once during testing. It is the most basic level of coverage. Branch Coverage : Tests each branch of decision points (e.g., every true/false path of an if statement). This ensures that all possible outcomes of conditional statements are exercised. Condition Coverage : It focuses on testing each individual condition within a decision point, ensuring that each condition has been evaluated as true or false. Modified Condition/Decision Coverage (MC/DC) : MC/DC ensures that each condition within a decision affects the outcome of that decision independently of other conditions. This reduces the number of test cases while still providing rigorous testing of complex conditional logic. Path Coverage : Ensures that all possible execution paths through the code are tested. This is more comprehensive than branch coverage as it accounts for all the possible execution sequences. Fun fact: MC/DC is frequently required by industry standards for safety-critical systems, such as aviation, medicine, and infrastructure. How to Apply MC/DC (Modified Condition/Decision Coverage) Identify the conditions and decision : List all individual conditions in the decision and identify the overall decision being evaluated Create a truth table : List all possible combinations of the conditions and calculate the decision outcome for each combination Determine independence pairs for each condition : Find pairs of test cases for each condition where only that condition changes value and the change causes the decision outcome to change Select the minimum set of test cases : Choose test cases that demonstrate the independent effect of each condition Verify coverage : Confirm each condition independently affects the decision Example Walkthrough of MC/DC Coverage For the decision if (A && (B || C)) : Test A B C Result 1 T T T T 2 T T F T 3 T F T T 4 T F F F 5 F T T F 6 F T F F 7 F F T F 8 F F F F Condition A : Test with A = true and A = false ensuring different outcomes, with B or C kept constant. Test case 1: A = true, B = true, C = true (Result: true ) Test case 5: A = false, B = true, C = true (Result: false ) Explanation: These test cases show A independently affecting the outcome when B and C are true. When A changes from true to false , the overall result is false , and it demonstrates A's independent effect. Other pairs of test cases: {2, 6}, {3, 7} Condition B : Test with B = true and B = false , ensuring different outcomes, with A or C constant. Test case 2: A = true, B = true, C = false (Result: true ) Test case 4: A = true, B = false, C = false (Result: false ) Explanation: These test cases demonstrate B's independent effect. When B changes from true to false , the overall result changes from true to false . No other pairs of tests. Condition C : Test with C = true and C = false , ensuring different outcomes, with A or B constant. Test case 3: A = true, B = false, C = true (Result: true ) Test case 4: A = true, B = false, C = false (Result: false ) Explanation: These test cases show C's independent effect. When C changes from true to false , the overall result changes from true to false . No other pairs of tests. Select Minimum Sets of Test Cases : The test cases {2, 3, 4, 6} exercise the independent effect of all conditions. {2, 6} exercise A {2, 4} exercise B {3, 4} exercise C So we only need to write four tests to ensure thorough coverage of this decision. Use this calculator to find minimal test cases needed to achieve MC/DC coverage: MC/DC Calculator Additional Notes Augmenting Specification-Based Testing : Structural testing should augment specification-based testing. This ensures that functional requirements are met and that the code behaves as expected in all scenarios. Using Source Code for Structural Testing : The source code is valuable for identifying test cases that may not be evident from the requirements alone. Structural testing leverages the code to uncover potential issues that specification-based testing might miss. Designing Contracts Designing contracts is a crucial aspect of software development that helps ensure the reliability and correctness of complex systems. This section will explore the concept of contracts, including pre-conditions, post-conditions, and invariants, and how they differ from validation. Understanding Contracts Contracts in software development are formal agreements between different parts of a program about their behaviour. They specify what conditions must be met before a method is called (pre-conditions), what conditions will be true after the method executes (post-conditions), and what conditions remain constant throughout the execution of a method or the lifetime of an object (invariants). Pre-conditions, Post-conditions, and Invariants Pre-conditions : Define what the method needs to function properly Ensure that input values received by the method adhere to what is required Example: A method that calculates square root might have a pre-condition that the input must be non-negative Post-conditions : Specify what the method guarantees as an outcome Ensure that the method returns what it promises to other methods Help detect bugs by throwing exceptions instead of returning invalid values Example: A method that sorts an array might have a post-condition that the returned array is in ascending order Invariants : Conditions that always hold true for a class or object Maintained throughout the execution of methods Example: A binary search tree class might have an invariant that the left child is always smaller than the parent node Contracts vs. Validation While both contracts and validation aim to ensure correct program behaviour, they serve different purposes: Contracts define the expected behaviour and interactions between different parts of a program. They are typically checked during development and may be disabled in production for performance reasons. Validation is about checking the correctness of data, usually at runtime, and is typically always active, even in production environments. Implementing Contracts There are several ways to implement contracts in code: Using assert keyword (in Java) : assert taxValue >= 0 : \"Calculated tax value cannot be negative!\"; Can be disabled via JVM parameter in production Use with caution if you don't have full control of your production environment Using conditional statements : if (taxValue < 0) { throw new IllegalStateException(\"Calculated tax value cannot be negative!\"); } More suitable when you need the checks to always be active Documentation : Clearly stating pre-conditions and post-conditions in method documentation is crucial and helps other developers understand how to use the method correctly. Strong vs. Weak Conditions Strong conditions halt execution when violated Weak conditions log errors but allow execution to continue Strong conditions are generally preferred as they reduce the range of potential errors in the code Example: Complex Financial System Consider a large software system handling complex financial processes: The system chains calls to several subroutines in a complex flow Results from one class are passed to the next class, and so on At some point, a TaxCalculator class is called Based on the requirements, calculations in TaxCalculator only make sense for positive numbers To handle this restriction: Define clear contracts for each class, including TaxCalculator Establish pre-conditions (e.g., input must be positive), post-conditions (e.g., calculated tax is non-negative), and invariants for the class Implement these contracts in the code using assertions or conditional checks Document the contracts clearly in the class and method documentation By implementing contracts, we can catch errors early, improve code reliability, and make the system's behaviour more predictable and maintainable. Property-Based Testing [TODO] Test Doubles and Mocks In software development, classes often depend on other classes to perform their functions. While testing multiple classes together can be desirable, testing a unit in isolation is frequently necessary without relying on its dependencies. This is where test doubles come into play. Consider a scenario where class A depends on classes B and C, which in turn have their own dependencies: A -> B -> Database -> C -> External service Testing A and its concrete dependencies might be too slow, too complex, or require too much setup. Test doubles allow us to isolate A and test it effectively. Types of Test Doubles Dummies : Simple objects passed to the class under test but never actually used They fulfil parameter requirements without impacting the test Fake Objects : Have working implementations of the simulated class, but in a simpler way Example: A fake database class using an ArrayList instead of a real database Stubs : Provide hard-coded answers to calls made during the test Don't have fully working implementations Most popular type of simulation Used when you need a dependency to return a specific value for the method under test to continue execution Mocks : Similar to stubs, but with additional capabilities Allow configuration of how they respond to method calls Record all interactions, enabling assertions on these interactions Spies : Wrap around real objects and observe their behaviour Used when the actual implementation is easier to use, but you still want to assert how the method under test interacts with the dependency Less common than other test doubles Advantages of Using Test Doubles Control : Test doubles provide complete control over the dependency's behaviour, allowing simulation of various scenarios Speed : Tests with doubles run faster as they bypass the overhead of real implementations Design Reflection : Using test doubles encourages developers to think critically about class interactions, leading to better-designed interfaces and contracts Introduction to Mockito Mocking frameworks like Mockito offer a simple API for setting up stubs and defining expectations on mock objects. Here are some crucial methods in Mockito: Creating a mock : MockObject mock = mock(ClassToMock.class); Defining stub behaviour : when(mock.someMethod()).thenReturn(someValue); Verifying interactions : verify(mock).someMethod(); Example using Mockito : // Create a mock List<String> mockedList = mock(List.class); // Define behavior when(mockedList.get(0)).thenReturn(\"first\"); // Use the mock System.out.println(mockedList.get(0)); // Outputs \"first\" // Verify interaction verify(mockedList, times(1)).get(0); Designing for Testability It's important to note that changing product code to make testing easier is acceptable and often encouraged. This practice can lead to improvements in the overall code quality and design. When you find your code difficult to test, it's usually a sign that the code could benefit from refactoring. Common changes include: Introducing interfaces for better abstraction Breaking down oversized methods into smaller, more focused ones Using dependency injection to make dependencies more flexible and more accessible to mock Remember, code that is easy to test is often more modular, has clearer responsibilities, and is generally of higher quality. Advanced Mocking Techniques Argument Capturing Argument capturing allows us to inspect arguments passed to mock methods. This is particularly useful when verifying complex objects passed to a method or when the method doesn't return a value. Simulating Exceptions Simulating exceptions with mocks helps us test how the system would behave in unexpected scenarios. This is particularly valuable when your application interacts with external systems that may not always behave as expected. @Test void handleDatabaseConnectionFailure() { // Arrange DatabaseService mockDb = mock(DatabaseService.class); when(mockDb.connect()).thenThrow(new ConnectionException(\"Database unavailable\")); MyService service = new MyService(mockDb); // Act & Assert that our service handles a database connection failure // correctly translate failure into ServiceUnavailableException assertThrows(ServiceUnavailableException.class, () -> { service.performCriticalOperation(); }); } Best Practices and Common Pitfalls Limit Mocking : Don't overuse mocks. Tests with real dependencies are often more effective at finding real bugs Mock Wisely : Good candidates for mocking include slow dependencies, external infrastructure, and cases that are hard to simulate (e.g., exceptions) Avoid Mocking : Entities, value objects, or classes that represent business concepts; native Java libraries and utility methods; simple objects that are easy to instantiate Keep Tests Clean : Use helper methods or classes to encapsulate common setup logic Design Stable Contracts : For mocks to work effectively in large codebases, design careful and stable contracts between classes Be Aware of Coupling : Mocks can increase coupling between test and production code. Be mindful of this when designing your tests Handling Unmockable Components Some components, like static methods or date/time operations, can be challenging to mock. Strategies for dealing with these include: Wrapper Classes Encapsulate unmockable components in wrapper classes that can be easily mocked. Example for date and time operations: public class Clock { public LocalDateTime now() { return LocalDateTime.now(); } } Dependency Injection Dependency Injection (DI) is a design pattern that makes it easier to test classes by allowing dependencies to be easily swapped out: // Without Dependency Injection public class UserService { private DatabaseConnection db = new DatabaseConnection(); public User getUser(int id) { return db.fetchUser(id); } } // With Dependency Injection public class UserService { private DatabaseConnection db; // Constructor Injection public UserService(DatabaseConnection db) { this.db = db; } public User getUser(int id) { return db.fetchUser(id); } } // In your test @Test void testGetUser() { // Arrange DatabaseConnection mockDb = mock(DatabaseConnection.class); when(mockDb.fetchUser(1)).thenReturn(new User(1, \"John Doe\")); UserService userService = new UserService(mockDb); // Act User user = userService.getUser(1); // Assert assertEquals(\"John Doe\", user.getName()); verify(mockDb).fetchUser(1); } Mocking Static Methods Mockito 3.4.0 and later versions provide support for mocking static methods: import static org.mockito.Mockito.*; import static org.mockito.ArgumentMatchers.*; class MyServiceTest { @Test void testStaticMethodMocking() { try (MockedStatic<UtilityClass> mockedStatic = mockStatic(UtilityClass.class)) { // Arrange mockedStatic.when(() -> UtilityClass.staticMethod(anyString())) .thenReturn(\"mocked result\"); // Act String result = MyService.methodUsingStaticUtility(\"input\"); // Assert assertEquals(\"expected result\", result); mockedStatic.verify(() -> UtilityClass.staticMethod(\"input\")); } } } Conclusion Test doubles and mocks are powerful tools for creating effective unit tests. By understanding when and how to use them, developers can create robust test suites that verify code correctness and contribute to better software design. Remember to balance the use of test doubles with tests that use real dependencies to ensure comprehensive coverage and realistic testing scenarios. By incorporating these advanced techniques and design principles, you can create more robust and flexible tests. Remember that the goal is not just to increase test coverage but to improve the overall design and reliability of our software. Test-Driven Development Test-driven development (TDD) is a software development methodology in which tests are written before the actual code implementation. This approach inverts the traditional development process, where code is typically implemented first and tested later. TDD emphasizes writing automated test cases for each piece of functionality before writing the code to implement that functionality. TDD Process The TDD process consists of three primary steps, often referred to as the Red-Green-Refactor cycle: Write a Test (Red) : Begin by writing an automated test for the next piece of functionality. Initially, this test will fail because the required implementation does not yet exist Implement the Functionality (Green) : Write the minimum code necessary to pass the test. The focus is on implementing just enough to meet the test's expectations Refactor : Clean up and improve the production and test code while ensuring all tests continue to pass. Refactoring should reduce the amount of code needed to satisfy the tests and improve the production code's readability, maintainability, and design These steps are repeated iteratively until the full functionality is implemented and all tests pass. Advantages of TDD Adopting a test-driven development approach can provide several benefits: Improved Code Quality : By writing tests first, TDD encourages better design decisions and helps catch defects early in the development process, leading to higher-quality code Rapid Feedback : The tight feedback loop provided by TDD allows developers to quickly identify and address issues, preventing them from compounding over time Enhanced Collaboration : TDD fosters collaboration among developers, testers, and business analysts by ensuring alignment on requirements and test scenarios, reducing misunderstandings and improving team communication Cost Efficiency : By reducing defects and improving code quality, TDD can lead to lower maintenance costs and a reduced total cost of ownership for the software project Improved Code Design : Writing tests first can expose design issues early, as the test code often acts as the first client of the class or component being developed What Does Research Say About TDD? The research on the effectiveness of TDD has produced mixed results: Janzen (2005) found that TDD practitioners produced less complex algorithms and more comprehensive test suites than non-TDD practitioners Janzen and Saiedian (2006) observed that TDD led to better use of object-oriented concepts and more effective class responsibility distribution George and Williams (2003) reported that while TDD initially reduced productivity for inexperienced developers, 92% of developers found it improved code quality Dog\u0161a and Bati\u010d (2011) concluded that TDD improved class design due to its simplicity Erdogmus et al. (2005) found that TDD increased productivity but did not significantly change code quality Nagappan et al. (2008) observed that TDD reduced pre-release defect density by 40-90% compared to projects that did not use TDD M\u00fcller and Hagner (2002) did not find that TDD accelerated implementation or improved reliability compared to traditional approaches Siniaalto and Abrahamsson (2007) found that TDD's benefits were unclear in small-scale projects Shull et al. (2010) reviewed multiple studies on TDD and did not observe a consistent effect on internal code quality Recent studies suggest that the iterative nature of TDD, rather than TDD itself, contributes to improved focus and flow, which can ultimately impact code quality. When Not to Use TDD While TDD is a valuable technique in many software development contexts, there are situations where it may not be the most appropriate approach: Well-understood Problems : If the problem and its solution are well-understood, writing tests before coding may add little value. Direct coding might be more efficient in such cases, though tests should still be written promptly Lack of Test Automation : TDD relies heavily on automated testing to provide quick feedback. If automated testing is not feasible, the TDD approach may not be practical Integrating TDD and Proper Testing It's important to note that TDD is not solely focused on testing; rather, it aids in developing production code by ensuring it meets requirements through automated test cases. Once the TDD cycle is complete, engaging in effective and systematic testing, such as specification-based and structural testing, is essential to ensure comprehensive coverage. The tests created during the TDD process become part of the overall testing suite. Example Walkthrough Consider this problem: /** * Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. * * You may assume that each input would have exactly one solution, and you may not use the same element twice. * * You can return the answer in any order. * * Example1: * Input: nums = [2, 7, 11, 15], target = 9 * Output: [0, 1] * Explanation: Because nums[0] + nums[1] == 0, we return [0, 1] * * Example2: * Input: nums = [3,3], target = 6 * Output: [0,1] * * Constraints: * <ul> * <li> 2 <= nums.length <= 104 </li> * <li> -109 <= nums[i] <= 109 </li> * <li> -109 <= target <= 109 </li> * </ul> */ public int[] twoSum(int[] nums, int target) { return null; } \ud83d\udd34 Red Phase Let's start by devising test cases that check the method constraints. import org.example.Main; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.params.ParameterizedTest; import org.junit.jupiter.params.provider.Arguments; import org.junit.jupiter.params.provider.MethodSource; import java.util.stream.IntStream; import java.util.stream.Stream; import static org.junit.jupiter.api.Assertions.assertThrows; public class MainTest { private Main main = new Main(); static Stream<Arguments> invalidInputProvider() { // nums length < 2 int[] nums1 = {1}; Arguments arguments1 = Arguments.of(nums1, 1); // nums length > 104 int[] nums2 = IntStream.range(0, 105).toArray(); Arguments arguments2 = Arguments.of(nums2, 1); // -109 < num[i] int[] nums3 = {1, 6, 4, -110, 2, 3}; Arguments arguments3 = Arguments.of(nums3, 1); // nums[i] > 109 int[] nums4 = {2, 4, 6, 109, 110}; Arguments arguments4 = Arguments.of(nums4, 1); // target < -109 int[] nums5 = {-100, -10}; int target5 = -110; Arguments arguments5 = Arguments.of(nums5, target5); // target > 110 int[] nums6 = {55, 11, 67}; int target6 = 110; Arguments arguments6 = Arguments.of(nums6, target6); return Stream.of( arguments1, arguments2, arguments3, arguments4, arguments5, arguments6 ); } @ParameterizedTest @MethodSource(\"invalidInputProvider\") @DisplayName(\"Test invalid method inputs\") void twoSum_breakMethodConstraints_shouldFail(int[] nums, int target) { assertThrows(IllegalArgumentException.class, () -> { main.twoSum(nums, target); }); } } This first test forces us to define what should happen if the input is invalid, a case not specified in the method requirements. We decided to throw an IllegalArgumentException . \ud83d\udfe2 Green Phase Implementing the solution is straightforward. We only need to add the method preconditions. public int[] twoSum(int[] nums, int target) { if (nums.length < 2 || nums.length > 104) { throw new IllegalArgumentException(\"Nums size is not valid!\"); } if (Arrays.stream(nums).anyMatch(num -> num < -109 || num > 109)) { throw new IllegalArgumentException(\"Nums element is not in bounds!\"); } if (target < -109 || target > 109) { throw new IllegalArgumentException(\"Target is not valid number!\"); } return null; } \u26ab Refactor Phase For refactoring, one could consider extracting the validation checks to a separate method, but we will not do that as the method is not too complex yet. The process continues iteratively: \ud83d\udd34 Red Phase (Next Cycle) In the next cycle, we'll extend the test suite to verify that the method actually outputs the correct values. static Stream<Arguments> correctValuesAndOutputProvider() { // simple length 2 input array int[] nums1 = {-109, 109}; int target1 = 0; int[] expected1 = {0, 1}; Arguments arguments1 = Arguments.of(nums1, target1, expected1); // length 3 input array int[] nums2 = {5, 17, 92}; int target2 = 109; int[] expected2 = {1, 2}; Arguments arguments2 = Arguments.of(nums2, target2, expected2); // length 4 input array int[] nums3 = {1, 6, 6, 10}; int target3 = 12; int[] expected3 = {1, 2}; Arguments arguments3 = Arguments.of(nums3, target3, expected3); // length 104 input array int[] nums4 = new int[104]; for (int i = 0; i < 102; i++) { nums4[i] = 0; } nums4[102] = 9; nums4[103] = 10; int target4 = 19; int[] expected4 = {102, 103}; Arguments arguments4 = Arguments.of(nums4, target4, expected4); return Stream.of( arguments1, arguments2, arguments3, arguments4 ); } @ParameterizedTest @MethodSource(\"correctValuesAndOutputProvider\") @DisplayName(\"Test correct method output\") void twoSum_correctInput_correctOutput(int[] nums, int target, int[] expected) { int[] result = main.twoSum(nums, target); Arrays.sort(expected); Arrays.sort(result); assertArrayEquals(expected, result); } \ud83d\udfe2 Green Phase Implementing the solution that passes the tests. public int[] twoSum(int[] nums, int target) { if (nums.length < 2 || nums.length > 104) { throw new IllegalArgumentException(\"Nums size is not valid!\"); } if (Arrays.stream(nums).anyMatch(num -> num < -109 || num > 109)) { throw new IllegalArgumentException(\"Nums element is not in bounds!\"); } if (target < -109 || target > 109) { throw new IllegalArgumentException(\"Target is not valid number!\"); } int[] result = new int[2]; outterLoop: for (int i = 0; i < nums.length - 1; i++) { for (int j = i+1; j < nums.length; j++) { if (nums[i] + nums[j] == target) { result[0] = i; result[1] = j; break outterLoop; } } } return result; } \u26ab Refactor Phase \ud83d\ude1b\ud83d\ude1b \ud83d\udd34 Red Phase (Final Cycle) Next, let's test the case where the method can't find a result. @Test void twoSum_correctInput_cantFindResult() { int[] nums = {1, 3, 5, 7}; int target = 5; assertNull(main.twoSum(nums, target)); } \ud83d\udfe2 Green Phase Updating the implementation to handle this case. public int[] twoSum(int[] nums, int target) { if (nums.length < 2 || nums.length > 104) { throw new IllegalArgumentException(\"Nums size is not valid!\"); } if (Arrays.stream(nums).anyMatch(num -> num < -109 || num > 109)) { throw new IllegalArgumentException(\"Nums element is not in bounds!\"); } if (target < -109 || target > 109) { throw new IllegalArgumentException(\"Target is not valid number!\"); } int[] result = null; outterLoop: for (int i = 0; i < nums.length - 1; i++) { for (int j = i+1; j < nums.length; j++) { if (nums[i] + nums[j] == target) { result = new int[]{i, j}; break outterLoop; } } } return result; } \u26ab Final Refactor Now, the method meets the expected functionality. We could consider rewriting the nested loop, adding post-conditions, or improving the method runtime ( O(n) is possible) for further refactoring. These are left as exercises for the reader. TODO Writing Larger Tests [TODO] Test Code Quality [TODO] This guide provides a comprehensive overview of effective software testing practices. For more advanced topics and specific implementation details, consider referring to additional resources and the original book by Maur\u00edcio Aniche.","title":"Effective Software Testing"},{"location":"tutorials/effective-software-testing/#effective-software-testing-a-developers-guide","text":"Effective and systematic software testing is critical for ensuring the reliability and quality of software systems. This guide provides a comprehensive overview of best code testing practices, inspired by the book 'Effective Software Testing: A Developer's Guide' by the software quality and testing expert Maur\u00edcio Aniche \ud83d\udc10. The goal is to help developers understand the importance of proper testing, how to approach it systematically, and how to implement it effectively in their projects.","title":"Effective Software Testing: A Developer's Guide"},{"location":"tutorials/effective-software-testing/#why-testing-is-important","text":"Quality Assurance : Testing is essential because software failures can have severe consequences for businesses and end-users. Developers bear responsibility for the quality of the software they produce. Bug Prevention : While simplicity in code design can reduce bugs, it cannot eliminate them. Testing is necessary to catch errors that arise from complex interactions within the software. Cost-Efficiency : The cost of fixing bugs in production is often higher than the cost of thorough testing during development. Systematic testing reduces the risk of releasing buggy software, saving time and resources in the long run. Code Improvement : Testing often leads to improvements in code quality. As developers write tests, they frequently identify areas where the code can be refactored for better clarity, efficiency, or maintainability. This process of writing tests and refactoring code results in more readable, maintainable, and robust software. Confidence in Changes : A comprehensive test suite gives developers confidence to make changes or add new features, knowing that existing functionality can be quickly verified.","title":"Why Testing is Important"},{"location":"tutorials/effective-software-testing/#the-testing-pyramid","text":"","title":"The Testing Pyramid"},{"location":"tutorials/effective-software-testing/#unit-testing","text":"Focus : Tests individual units (e.g., methods or functions) in isolation Benefits : Fast, easy to write, and control Limitations : May not catch bugs that arise from interactions between units","title":"Unit Testing"},{"location":"tutorials/effective-software-testing/#integration-testing","text":"Focus : Tests interactions between different units or with external systems Benefits : Identifies issues in the integration layer Limitations : More complex and time-consuming to write","title":"Integration Testing"},{"location":"tutorials/effective-software-testing/#system-testing","text":"Focus : Tests the entire system as a whole, including all components and their interactions Benefits : Provides a realistic view of how the system behaves in production Limitations : Slower to execute, more prone to flaky results, and harder to write","title":"System Testing"},{"location":"tutorials/effective-software-testing/#when-to-use-each","text":"Unit Tests : For all business rules and logic Integration Tests : For complex integrations with external services System Tests : For critical workflows and high-risk areas of the application","title":"When to Use Each"},{"location":"tutorials/effective-software-testing/#specification-based-testing","text":"Specification-based testing derives test cases from the software's requirements, ensuring the code meets its intended functionality.","title":"Specification-Based Testing"},{"location":"tutorials/effective-software-testing/#steps-for-specification-based-testing","text":"Understand the Requirements : Identify what the software should and should not do based on requirements or user stories Explore the Program : Investigate how the software handles various inputs and outputs Identify Partitions : Group inputs into partitions where the software is expected to behave similarly Identify Boundaries : Focus on edge cases, such as values around decision points (e.g., >, <, == operators) Devise Test Cases : Create tests based on identified partitions and boundaries, prioritizing edge cases and unique combinations Automate Test Cases : Implement tests using tools like JUnit, ensuring tests are easy to read and maintain Augment with Creativity : Use experience and intuition to identify additional test cases that may not be directly derived from the specifications","title":"Steps for Specification-Based Testing"},{"location":"tutorials/effective-software-testing/#example-walkthrough-user-registration-function","text":"Consider a user registration function with a username and password: Requirements : The username must be 3-20 characters long and contain only alphanumeric characters. The password must be at least 8 characters long and contain at least one number, one capital letter, and one symbol. Partitions : Username: null, too short, too long, invalid, valid Password: null, too short, invalid, valid Boundaries : The test cases will include usernames of different lengths, from the shortest (2 characters\u2014invalid, 3 characters\u2014valid) to the longest (20 characters\u2014valid, 21 characters\u2014invalid). The same applies to password length: 7 characters\u2014invalid, 8 characters\u2014valid. Test Cases : Test an empty username Test an empty password Test a valid username that is too short Test an invalid username of 3 and 20 characters Test a valid username Test a valid username that is too short Test a valid password that is too short Test an invalid password of 8 characters Test a valid password Augment with Experience : We might know from experience that users may attempt to use symbols in their passwords that could pose security risks (e.g., backticks (`), pipe (|), angle brackets (<, >), etc.). Test cases should be added to ensure that passwords containing such characters are deemed invalid.","title":"Example Walkthrough: User Registration Function"},{"location":"tutorials/effective-software-testing/#structural-testing-and-code-coverage","text":"Structural testing, also known as white-box testing, involves testing the internal structure of the code to ensure thorough coverage of all its components. Unlike specification-based (black-box) testing, which focuses on functional requirements, structural testing delves into the code to ensure that all paths, branches, and conditions are adequately tested.","title":"Structural Testing and Code Coverage"},{"location":"tutorials/effective-software-testing/#steps-for-structural-testing","text":"Perform Specification-Based Testing : Begin with tests derived from functional requirements to validate expected behaviour Review Code : Carefully read the unit implementation to understand the developer's key coding decisions Run Tests with Coverage Tools : Execute the devised test suite using code coverage tools Analyze Coverage Gaps : Identify any untested code segments revealed by the coverage analysis Decide on Testing Missed Code : Evaluate whether the untested code is significant enough to warrant additional testing Iterate and Refine : Revisit the source code to identify other test cases that could be derived from the code structure","title":"Steps for Structural Testing"},{"location":"tutorials/effective-software-testing/#code-coverage-criteria","text":"Line Coverage : Ensures that every line of code is executed at least once during testing. It is the most basic level of coverage. Branch Coverage : Tests each branch of decision points (e.g., every true/false path of an if statement). This ensures that all possible outcomes of conditional statements are exercised. Condition Coverage : It focuses on testing each individual condition within a decision point, ensuring that each condition has been evaluated as true or false. Modified Condition/Decision Coverage (MC/DC) : MC/DC ensures that each condition within a decision affects the outcome of that decision independently of other conditions. This reduces the number of test cases while still providing rigorous testing of complex conditional logic. Path Coverage : Ensures that all possible execution paths through the code are tested. This is more comprehensive than branch coverage as it accounts for all the possible execution sequences. Fun fact: MC/DC is frequently required by industry standards for safety-critical systems, such as aviation, medicine, and infrastructure.","title":"Code Coverage Criteria"},{"location":"tutorials/effective-software-testing/#how-to-apply-mcdc-modified-conditiondecision-coverage","text":"Identify the conditions and decision : List all individual conditions in the decision and identify the overall decision being evaluated Create a truth table : List all possible combinations of the conditions and calculate the decision outcome for each combination Determine independence pairs for each condition : Find pairs of test cases for each condition where only that condition changes value and the change causes the decision outcome to change Select the minimum set of test cases : Choose test cases that demonstrate the independent effect of each condition Verify coverage : Confirm each condition independently affects the decision","title":"How to Apply MC/DC (Modified Condition/Decision Coverage)"},{"location":"tutorials/effective-software-testing/#example-walkthrough-of-mcdc-coverage","text":"For the decision if (A && (B || C)) : Test A B C Result 1 T T T T 2 T T F T 3 T F T T 4 T F F F 5 F T T F 6 F T F F 7 F F T F 8 F F F F Condition A : Test with A = true and A = false ensuring different outcomes, with B or C kept constant. Test case 1: A = true, B = true, C = true (Result: true ) Test case 5: A = false, B = true, C = true (Result: false ) Explanation: These test cases show A independently affecting the outcome when B and C are true. When A changes from true to false , the overall result is false , and it demonstrates A's independent effect. Other pairs of test cases: {2, 6}, {3, 7} Condition B : Test with B = true and B = false , ensuring different outcomes, with A or C constant. Test case 2: A = true, B = true, C = false (Result: true ) Test case 4: A = true, B = false, C = false (Result: false ) Explanation: These test cases demonstrate B's independent effect. When B changes from true to false , the overall result changes from true to false . No other pairs of tests. Condition C : Test with C = true and C = false , ensuring different outcomes, with A or B constant. Test case 3: A = true, B = false, C = true (Result: true ) Test case 4: A = true, B = false, C = false (Result: false ) Explanation: These test cases show C's independent effect. When C changes from true to false , the overall result changes from true to false . No other pairs of tests. Select Minimum Sets of Test Cases : The test cases {2, 3, 4, 6} exercise the independent effect of all conditions. {2, 6} exercise A {2, 4} exercise B {3, 4} exercise C So we only need to write four tests to ensure thorough coverage of this decision. Use this calculator to find minimal test cases needed to achieve MC/DC coverage: MC/DC Calculator","title":"Example Walkthrough of MC/DC Coverage"},{"location":"tutorials/effective-software-testing/#additional-notes","text":"Augmenting Specification-Based Testing : Structural testing should augment specification-based testing. This ensures that functional requirements are met and that the code behaves as expected in all scenarios. Using Source Code for Structural Testing : The source code is valuable for identifying test cases that may not be evident from the requirements alone. Structural testing leverages the code to uncover potential issues that specification-based testing might miss.","title":"Additional Notes"},{"location":"tutorials/effective-software-testing/#designing-contracts","text":"Designing contracts is a crucial aspect of software development that helps ensure the reliability and correctness of complex systems. This section will explore the concept of contracts, including pre-conditions, post-conditions, and invariants, and how they differ from validation.","title":"Designing Contracts"},{"location":"tutorials/effective-software-testing/#understanding-contracts","text":"Contracts in software development are formal agreements between different parts of a program about their behaviour. They specify what conditions must be met before a method is called (pre-conditions), what conditions will be true after the method executes (post-conditions), and what conditions remain constant throughout the execution of a method or the lifetime of an object (invariants).","title":"Understanding Contracts"},{"location":"tutorials/effective-software-testing/#pre-conditions-post-conditions-and-invariants","text":"Pre-conditions : Define what the method needs to function properly Ensure that input values received by the method adhere to what is required Example: A method that calculates square root might have a pre-condition that the input must be non-negative Post-conditions : Specify what the method guarantees as an outcome Ensure that the method returns what it promises to other methods Help detect bugs by throwing exceptions instead of returning invalid values Example: A method that sorts an array might have a post-condition that the returned array is in ascending order Invariants : Conditions that always hold true for a class or object Maintained throughout the execution of methods Example: A binary search tree class might have an invariant that the left child is always smaller than the parent node","title":"Pre-conditions, Post-conditions, and Invariants"},{"location":"tutorials/effective-software-testing/#contracts-vs-validation","text":"While both contracts and validation aim to ensure correct program behaviour, they serve different purposes: Contracts define the expected behaviour and interactions between different parts of a program. They are typically checked during development and may be disabled in production for performance reasons. Validation is about checking the correctness of data, usually at runtime, and is typically always active, even in production environments.","title":"Contracts vs. Validation"},{"location":"tutorials/effective-software-testing/#implementing-contracts","text":"There are several ways to implement contracts in code: Using assert keyword (in Java) : assert taxValue >= 0 : \"Calculated tax value cannot be negative!\"; Can be disabled via JVM parameter in production Use with caution if you don't have full control of your production environment Using conditional statements : if (taxValue < 0) { throw new IllegalStateException(\"Calculated tax value cannot be negative!\"); } More suitable when you need the checks to always be active Documentation : Clearly stating pre-conditions and post-conditions in method documentation is crucial and helps other developers understand how to use the method correctly.","title":"Implementing Contracts"},{"location":"tutorials/effective-software-testing/#strong-vs-weak-conditions","text":"Strong conditions halt execution when violated Weak conditions log errors but allow execution to continue Strong conditions are generally preferred as they reduce the range of potential errors in the code","title":"Strong vs. Weak Conditions"},{"location":"tutorials/effective-software-testing/#example-complex-financial-system","text":"Consider a large software system handling complex financial processes: The system chains calls to several subroutines in a complex flow Results from one class are passed to the next class, and so on At some point, a TaxCalculator class is called Based on the requirements, calculations in TaxCalculator only make sense for positive numbers To handle this restriction: Define clear contracts for each class, including TaxCalculator Establish pre-conditions (e.g., input must be positive), post-conditions (e.g., calculated tax is non-negative), and invariants for the class Implement these contracts in the code using assertions or conditional checks Document the contracts clearly in the class and method documentation By implementing contracts, we can catch errors early, improve code reliability, and make the system's behaviour more predictable and maintainable.","title":"Example: Complex Financial System"},{"location":"tutorials/effective-software-testing/#property-based-testing","text":"[TODO]","title":"Property-Based Testing"},{"location":"tutorials/effective-software-testing/#test-doubles-and-mocks","text":"In software development, classes often depend on other classes to perform their functions. While testing multiple classes together can be desirable, testing a unit in isolation is frequently necessary without relying on its dependencies. This is where test doubles come into play. Consider a scenario where class A depends on classes B and C, which in turn have their own dependencies: A -> B -> Database -> C -> External service Testing A and its concrete dependencies might be too slow, too complex, or require too much setup. Test doubles allow us to isolate A and test it effectively.","title":"Test Doubles and Mocks"},{"location":"tutorials/effective-software-testing/#types-of-test-doubles","text":"Dummies : Simple objects passed to the class under test but never actually used They fulfil parameter requirements without impacting the test Fake Objects : Have working implementations of the simulated class, but in a simpler way Example: A fake database class using an ArrayList instead of a real database Stubs : Provide hard-coded answers to calls made during the test Don't have fully working implementations Most popular type of simulation Used when you need a dependency to return a specific value for the method under test to continue execution Mocks : Similar to stubs, but with additional capabilities Allow configuration of how they respond to method calls Record all interactions, enabling assertions on these interactions Spies : Wrap around real objects and observe their behaviour Used when the actual implementation is easier to use, but you still want to assert how the method under test interacts with the dependency Less common than other test doubles","title":"Types of Test Doubles"},{"location":"tutorials/effective-software-testing/#advantages-of-using-test-doubles","text":"Control : Test doubles provide complete control over the dependency's behaviour, allowing simulation of various scenarios Speed : Tests with doubles run faster as they bypass the overhead of real implementations Design Reflection : Using test doubles encourages developers to think critically about class interactions, leading to better-designed interfaces and contracts","title":"Advantages of Using Test Doubles"},{"location":"tutorials/effective-software-testing/#introduction-to-mockito","text":"Mocking frameworks like Mockito offer a simple API for setting up stubs and defining expectations on mock objects. Here are some crucial methods in Mockito: Creating a mock : MockObject mock = mock(ClassToMock.class); Defining stub behaviour : when(mock.someMethod()).thenReturn(someValue); Verifying interactions : verify(mock).someMethod(); Example using Mockito : // Create a mock List<String> mockedList = mock(List.class); // Define behavior when(mockedList.get(0)).thenReturn(\"first\"); // Use the mock System.out.println(mockedList.get(0)); // Outputs \"first\" // Verify interaction verify(mockedList, times(1)).get(0);","title":"Introduction to Mockito"},{"location":"tutorials/effective-software-testing/#designing-for-testability","text":"It's important to note that changing product code to make testing easier is acceptable and often encouraged. This practice can lead to improvements in the overall code quality and design. When you find your code difficult to test, it's usually a sign that the code could benefit from refactoring. Common changes include: Introducing interfaces for better abstraction Breaking down oversized methods into smaller, more focused ones Using dependency injection to make dependencies more flexible and more accessible to mock Remember, code that is easy to test is often more modular, has clearer responsibilities, and is generally of higher quality.","title":"Designing for Testability"},{"location":"tutorials/effective-software-testing/#advanced-mocking-techniques","text":"","title":"Advanced Mocking Techniques"},{"location":"tutorials/effective-software-testing/#argument-capturing","text":"Argument capturing allows us to inspect arguments passed to mock methods. This is particularly useful when verifying complex objects passed to a method or when the method doesn't return a value.","title":"Argument Capturing"},{"location":"tutorials/effective-software-testing/#simulating-exceptions","text":"Simulating exceptions with mocks helps us test how the system would behave in unexpected scenarios. This is particularly valuable when your application interacts with external systems that may not always behave as expected. @Test void handleDatabaseConnectionFailure() { // Arrange DatabaseService mockDb = mock(DatabaseService.class); when(mockDb.connect()).thenThrow(new ConnectionException(\"Database unavailable\")); MyService service = new MyService(mockDb); // Act & Assert that our service handles a database connection failure // correctly translate failure into ServiceUnavailableException assertThrows(ServiceUnavailableException.class, () -> { service.performCriticalOperation(); }); }","title":"Simulating Exceptions"},{"location":"tutorials/effective-software-testing/#best-practices-and-common-pitfalls","text":"Limit Mocking : Don't overuse mocks. Tests with real dependencies are often more effective at finding real bugs Mock Wisely : Good candidates for mocking include slow dependencies, external infrastructure, and cases that are hard to simulate (e.g., exceptions) Avoid Mocking : Entities, value objects, or classes that represent business concepts; native Java libraries and utility methods; simple objects that are easy to instantiate Keep Tests Clean : Use helper methods or classes to encapsulate common setup logic Design Stable Contracts : For mocks to work effectively in large codebases, design careful and stable contracts between classes Be Aware of Coupling : Mocks can increase coupling between test and production code. Be mindful of this when designing your tests","title":"Best Practices and Common Pitfalls"},{"location":"tutorials/effective-software-testing/#handling-unmockable-components","text":"Some components, like static methods or date/time operations, can be challenging to mock. Strategies for dealing with these include:","title":"Handling Unmockable Components"},{"location":"tutorials/effective-software-testing/#wrapper-classes","text":"Encapsulate unmockable components in wrapper classes that can be easily mocked. Example for date and time operations: public class Clock { public LocalDateTime now() { return LocalDateTime.now(); } }","title":"Wrapper Classes"},{"location":"tutorials/effective-software-testing/#dependency-injection","text":"Dependency Injection (DI) is a design pattern that makes it easier to test classes by allowing dependencies to be easily swapped out: // Without Dependency Injection public class UserService { private DatabaseConnection db = new DatabaseConnection(); public User getUser(int id) { return db.fetchUser(id); } } // With Dependency Injection public class UserService { private DatabaseConnection db; // Constructor Injection public UserService(DatabaseConnection db) { this.db = db; } public User getUser(int id) { return db.fetchUser(id); } } // In your test @Test void testGetUser() { // Arrange DatabaseConnection mockDb = mock(DatabaseConnection.class); when(mockDb.fetchUser(1)).thenReturn(new User(1, \"John Doe\")); UserService userService = new UserService(mockDb); // Act User user = userService.getUser(1); // Assert assertEquals(\"John Doe\", user.getName()); verify(mockDb).fetchUser(1); }","title":"Dependency Injection"},{"location":"tutorials/effective-software-testing/#mocking-static-methods","text":"Mockito 3.4.0 and later versions provide support for mocking static methods: import static org.mockito.Mockito.*; import static org.mockito.ArgumentMatchers.*; class MyServiceTest { @Test void testStaticMethodMocking() { try (MockedStatic<UtilityClass> mockedStatic = mockStatic(UtilityClass.class)) { // Arrange mockedStatic.when(() -> UtilityClass.staticMethod(anyString())) .thenReturn(\"mocked result\"); // Act String result = MyService.methodUsingStaticUtility(\"input\"); // Assert assertEquals(\"expected result\", result); mockedStatic.verify(() -> UtilityClass.staticMethod(\"input\")); } } }","title":"Mocking Static Methods"},{"location":"tutorials/effective-software-testing/#conclusion","text":"Test doubles and mocks are powerful tools for creating effective unit tests. By understanding when and how to use them, developers can create robust test suites that verify code correctness and contribute to better software design. Remember to balance the use of test doubles with tests that use real dependencies to ensure comprehensive coverage and realistic testing scenarios. By incorporating these advanced techniques and design principles, you can create more robust and flexible tests. Remember that the goal is not just to increase test coverage but to improve the overall design and reliability of our software.","title":"Conclusion"},{"location":"tutorials/effective-software-testing/#test-driven-development","text":"Test-driven development (TDD) is a software development methodology in which tests are written before the actual code implementation. This approach inverts the traditional development process, where code is typically implemented first and tested later. TDD emphasizes writing automated test cases for each piece of functionality before writing the code to implement that functionality.","title":"Test-Driven Development"},{"location":"tutorials/effective-software-testing/#tdd-process","text":"The TDD process consists of three primary steps, often referred to as the Red-Green-Refactor cycle: Write a Test (Red) : Begin by writing an automated test for the next piece of functionality. Initially, this test will fail because the required implementation does not yet exist Implement the Functionality (Green) : Write the minimum code necessary to pass the test. The focus is on implementing just enough to meet the test's expectations Refactor : Clean up and improve the production and test code while ensuring all tests continue to pass. Refactoring should reduce the amount of code needed to satisfy the tests and improve the production code's readability, maintainability, and design These steps are repeated iteratively until the full functionality is implemented and all tests pass.","title":"TDD Process"},{"location":"tutorials/effective-software-testing/#advantages-of-tdd","text":"Adopting a test-driven development approach can provide several benefits: Improved Code Quality : By writing tests first, TDD encourages better design decisions and helps catch defects early in the development process, leading to higher-quality code Rapid Feedback : The tight feedback loop provided by TDD allows developers to quickly identify and address issues, preventing them from compounding over time Enhanced Collaboration : TDD fosters collaboration among developers, testers, and business analysts by ensuring alignment on requirements and test scenarios, reducing misunderstandings and improving team communication Cost Efficiency : By reducing defects and improving code quality, TDD can lead to lower maintenance costs and a reduced total cost of ownership for the software project Improved Code Design : Writing tests first can expose design issues early, as the test code often acts as the first client of the class or component being developed","title":"Advantages of TDD"},{"location":"tutorials/effective-software-testing/#what-does-research-say-about-tdd","text":"The research on the effectiveness of TDD has produced mixed results: Janzen (2005) found that TDD practitioners produced less complex algorithms and more comprehensive test suites than non-TDD practitioners Janzen and Saiedian (2006) observed that TDD led to better use of object-oriented concepts and more effective class responsibility distribution George and Williams (2003) reported that while TDD initially reduced productivity for inexperienced developers, 92% of developers found it improved code quality Dog\u0161a and Bati\u010d (2011) concluded that TDD improved class design due to its simplicity Erdogmus et al. (2005) found that TDD increased productivity but did not significantly change code quality Nagappan et al. (2008) observed that TDD reduced pre-release defect density by 40-90% compared to projects that did not use TDD M\u00fcller and Hagner (2002) did not find that TDD accelerated implementation or improved reliability compared to traditional approaches Siniaalto and Abrahamsson (2007) found that TDD's benefits were unclear in small-scale projects Shull et al. (2010) reviewed multiple studies on TDD and did not observe a consistent effect on internal code quality Recent studies suggest that the iterative nature of TDD, rather than TDD itself, contributes to improved focus and flow, which can ultimately impact code quality.","title":"What Does Research Say About TDD?"},{"location":"tutorials/effective-software-testing/#when-not-to-use-tdd","text":"While TDD is a valuable technique in many software development contexts, there are situations where it may not be the most appropriate approach: Well-understood Problems : If the problem and its solution are well-understood, writing tests before coding may add little value. Direct coding might be more efficient in such cases, though tests should still be written promptly Lack of Test Automation : TDD relies heavily on automated testing to provide quick feedback. If automated testing is not feasible, the TDD approach may not be practical","title":"When Not to Use TDD"},{"location":"tutorials/effective-software-testing/#integrating-tdd-and-proper-testing","text":"It's important to note that TDD is not solely focused on testing; rather, it aids in developing production code by ensuring it meets requirements through automated test cases. Once the TDD cycle is complete, engaging in effective and systematic testing, such as specification-based and structural testing, is essential to ensure comprehensive coverage. The tests created during the TDD process become part of the overall testing suite.","title":"Integrating TDD and Proper Testing"},{"location":"tutorials/effective-software-testing/#example-walkthrough","text":"Consider this problem: /** * Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. * * You may assume that each input would have exactly one solution, and you may not use the same element twice. * * You can return the answer in any order. * * Example1: * Input: nums = [2, 7, 11, 15], target = 9 * Output: [0, 1] * Explanation: Because nums[0] + nums[1] == 0, we return [0, 1] * * Example2: * Input: nums = [3,3], target = 6 * Output: [0,1] * * Constraints: * <ul> * <li> 2 <= nums.length <= 104 </li> * <li> -109 <= nums[i] <= 109 </li> * <li> -109 <= target <= 109 </li> * </ul> */ public int[] twoSum(int[] nums, int target) { return null; }","title":"Example Walkthrough"},{"location":"tutorials/effective-software-testing/#red-phase","text":"Let's start by devising test cases that check the method constraints. import org.example.Main; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.params.ParameterizedTest; import org.junit.jupiter.params.provider.Arguments; import org.junit.jupiter.params.provider.MethodSource; import java.util.stream.IntStream; import java.util.stream.Stream; import static org.junit.jupiter.api.Assertions.assertThrows; public class MainTest { private Main main = new Main(); static Stream<Arguments> invalidInputProvider() { // nums length < 2 int[] nums1 = {1}; Arguments arguments1 = Arguments.of(nums1, 1); // nums length > 104 int[] nums2 = IntStream.range(0, 105).toArray(); Arguments arguments2 = Arguments.of(nums2, 1); // -109 < num[i] int[] nums3 = {1, 6, 4, -110, 2, 3}; Arguments arguments3 = Arguments.of(nums3, 1); // nums[i] > 109 int[] nums4 = {2, 4, 6, 109, 110}; Arguments arguments4 = Arguments.of(nums4, 1); // target < -109 int[] nums5 = {-100, -10}; int target5 = -110; Arguments arguments5 = Arguments.of(nums5, target5); // target > 110 int[] nums6 = {55, 11, 67}; int target6 = 110; Arguments arguments6 = Arguments.of(nums6, target6); return Stream.of( arguments1, arguments2, arguments3, arguments4, arguments5, arguments6 ); } @ParameterizedTest @MethodSource(\"invalidInputProvider\") @DisplayName(\"Test invalid method inputs\") void twoSum_breakMethodConstraints_shouldFail(int[] nums, int target) { assertThrows(IllegalArgumentException.class, () -> { main.twoSum(nums, target); }); } } This first test forces us to define what should happen if the input is invalid, a case not specified in the method requirements. We decided to throw an IllegalArgumentException .","title":"\ud83d\udd34 Red Phase"},{"location":"tutorials/effective-software-testing/#green-phase","text":"Implementing the solution is straightforward. We only need to add the method preconditions. public int[] twoSum(int[] nums, int target) { if (nums.length < 2 || nums.length > 104) { throw new IllegalArgumentException(\"Nums size is not valid!\"); } if (Arrays.stream(nums).anyMatch(num -> num < -109 || num > 109)) { throw new IllegalArgumentException(\"Nums element is not in bounds!\"); } if (target < -109 || target > 109) { throw new IllegalArgumentException(\"Target is not valid number!\"); } return null; }","title":"\ud83d\udfe2 Green Phase"},{"location":"tutorials/effective-software-testing/#refactor-phase","text":"For refactoring, one could consider extracting the validation checks to a separate method, but we will not do that as the method is not too complex yet. The process continues iteratively:","title":"\u26ab Refactor Phase"},{"location":"tutorials/effective-software-testing/#red-phase-next-cycle","text":"In the next cycle, we'll extend the test suite to verify that the method actually outputs the correct values. static Stream<Arguments> correctValuesAndOutputProvider() { // simple length 2 input array int[] nums1 = {-109, 109}; int target1 = 0; int[] expected1 = {0, 1}; Arguments arguments1 = Arguments.of(nums1, target1, expected1); // length 3 input array int[] nums2 = {5, 17, 92}; int target2 = 109; int[] expected2 = {1, 2}; Arguments arguments2 = Arguments.of(nums2, target2, expected2); // length 4 input array int[] nums3 = {1, 6, 6, 10}; int target3 = 12; int[] expected3 = {1, 2}; Arguments arguments3 = Arguments.of(nums3, target3, expected3); // length 104 input array int[] nums4 = new int[104]; for (int i = 0; i < 102; i++) { nums4[i] = 0; } nums4[102] = 9; nums4[103] = 10; int target4 = 19; int[] expected4 = {102, 103}; Arguments arguments4 = Arguments.of(nums4, target4, expected4); return Stream.of( arguments1, arguments2, arguments3, arguments4 ); } @ParameterizedTest @MethodSource(\"correctValuesAndOutputProvider\") @DisplayName(\"Test correct method output\") void twoSum_correctInput_correctOutput(int[] nums, int target, int[] expected) { int[] result = main.twoSum(nums, target); Arrays.sort(expected); Arrays.sort(result); assertArrayEquals(expected, result); }","title":"\ud83d\udd34 Red Phase (Next Cycle)"},{"location":"tutorials/effective-software-testing/#green-phase_1","text":"Implementing the solution that passes the tests. public int[] twoSum(int[] nums, int target) { if (nums.length < 2 || nums.length > 104) { throw new IllegalArgumentException(\"Nums size is not valid!\"); } if (Arrays.stream(nums).anyMatch(num -> num < -109 || num > 109)) { throw new IllegalArgumentException(\"Nums element is not in bounds!\"); } if (target < -109 || target > 109) { throw new IllegalArgumentException(\"Target is not valid number!\"); } int[] result = new int[2]; outterLoop: for (int i = 0; i < nums.length - 1; i++) { for (int j = i+1; j < nums.length; j++) { if (nums[i] + nums[j] == target) { result[0] = i; result[1] = j; break outterLoop; } } } return result; }","title":"\ud83d\udfe2 Green Phase"},{"location":"tutorials/effective-software-testing/#refactor-phase_1","text":"\ud83d\ude1b\ud83d\ude1b","title":"\u26ab Refactor Phase"},{"location":"tutorials/effective-software-testing/#red-phase-final-cycle","text":"Next, let's test the case where the method can't find a result. @Test void twoSum_correctInput_cantFindResult() { int[] nums = {1, 3, 5, 7}; int target = 5; assertNull(main.twoSum(nums, target)); }","title":"\ud83d\udd34 Red Phase (Final Cycle)"},{"location":"tutorials/effective-software-testing/#green-phase_2","text":"Updating the implementation to handle this case. public int[] twoSum(int[] nums, int target) { if (nums.length < 2 || nums.length > 104) { throw new IllegalArgumentException(\"Nums size is not valid!\"); } if (Arrays.stream(nums).anyMatch(num -> num < -109 || num > 109)) { throw new IllegalArgumentException(\"Nums element is not in bounds!\"); } if (target < -109 || target > 109) { throw new IllegalArgumentException(\"Target is not valid number!\"); } int[] result = null; outterLoop: for (int i = 0; i < nums.length - 1; i++) { for (int j = i+1; j < nums.length; j++) { if (nums[i] + nums[j] == target) { result = new int[]{i, j}; break outterLoop; } } } return result; }","title":"\ud83d\udfe2 Green Phase"},{"location":"tutorials/effective-software-testing/#final-refactor","text":"Now, the method meets the expected functionality. We could consider rewriting the nested loop, adding post-conditions, or improving the method runtime ( O(n) is possible) for further refactoring. These are left as exercises for the reader. TODO","title":"\u26ab Final Refactor"},{"location":"tutorials/effective-software-testing/#writing-larger-tests","text":"[TODO]","title":"Writing Larger Tests"},{"location":"tutorials/effective-software-testing/#test-code-quality","text":"[TODO] This guide provides a comprehensive overview of effective software testing practices. For more advanced topics and specific implementation details, consider referring to additional resources and the original book by Maur\u00edcio Aniche.","title":"Test Code Quality"},{"location":"tutorials/open-webui-action-tools/","text":"Hii","title":"Beyond Text. Equipping Your Open WebUI AI with Action Tools"},{"location":"tutorials/open-webui-rag/","text":"Open WebUI tutorial. Supercharge Your Local AI with RAG and Custom Knowledge Bases In this second part of our local AI series, we\u2019ll dive into Retrieval-Augmented Generation (RAG) , showing you how to dramatically improve your AI\u2019s capabilities by connecting it to your own documents and knowledge sources. By the end, you\u2019ll have an AI that can answer questions based on private data without sending anything to external servers. TODO: add pic Table of Contents: Introduction What is RAG and Why Should You Care? RAG Options in Open WebUI Optimizing Your RAG System Data Processing for Knowledge Base Creation Choosing the Right Model for RAG Creating the Perfect System Prompt for RAG Real-World RAG Examples Wikipedia Knowledge Base Open WebUI Documentation Assistant Conclusion Introduction In Part One, we set up a robust local AI environment with Ollama and Open WebUI, giving you a completely private, subscription-free alternative to commercial AI services. Today, we\u2019re taking this setup to the next level. While large language models (LLMs) are impressive, they come with two significant limitations: Their knowledge has a cutoff date (they don\u2019t know about recent events) They don\u2019t know about your private information (documents, data, etc.) This is where RAG comes in \u2014 a technique that allows your local AI to access, understand, and leverage custom knowledge sources to provide more accurate, personalized, and up-to-date responses. What you\u2019ll learn in this guide: Understanding how RAG works and why it matters How to set up and optimize RAG in Open WebUI. Real-world examples of RAG with practical applications. General tips and tricks for achieving the best results with your Open WebUI application. Let\u2019s transform your local AI from a general-purpose assistant into a specialized knowledge worker tailored to your specific needs. What is RAG and Why Should You Care? TODO: add pic Retrieval-Augmented Generation visualized RAG, or Retrieval-Augmented Generation, might sound technical. Still, the concept is straightforward: when your AI receives a question, it first searches your documents for relevant information. Then, it uses the retrieved data to generate a more informed response. Here\u2019s the magic of RAG in simple terms: Your question \u2192 \u201cCan you explain how our company\u2019s vacation policy works?\u201d The RAG system \u2192 Searches your company handbook for relevant sections Retrieved context \u2192 Found policy details on pages 24\u201325 The AI \u2192 Uses this retrieved information to answer accurately Without RAG, your AI would either make something up (hallucinate) or say it doesn\u2019t know. RAG can provide precise answers based on your actual documents. Pro Tip : RAG is particularly valuable for specialized knowledge work \u2014 think customer support, research assistance, or mining insights from technical documentation. Under the Hood: Vectors and Embeddings TODO: add pic At a technical level, RAG converts your documents into \u201cvectors\u201d \u2014 essentially mathematical representations of text that capture semantic meaning. When you upload documents to Open WebUI: The text is split into manageable chunks Each chunk is encoded into a vector using an embedding model These vectors are stored in a database optimized for similarity search When you ask a question, it\u2019s also converted to a vector The system finds the most similar document vectors to your question vector This vector-based approach allows the system to retrieve information based on meaning, not just keyword matching. RAG Options in Open WebUI Open WebUI offers several ways to implement RAG without writing a single line of code. Let\u2019s explore each approach: 1. Single Document Upload (Quick & Simple) The most straightforward approach is uploading a document directly to the chat interface. How to do it: Start a new chat in Open WebUI Click the upload button in the chat input area Select your document Ask questions related to your document TODO: add pic Best for: Quick, one-off questions about a specific document. 2. Web Information Retrieval Need information from a specific webpage? How to do it: In your chat, type # followed by a URL Open WebUI fetches and parses the webpage content Ask questions about the content Example : #https://yourcompany.com/blog/latest-product what features were announced? Best for : Getting information from specific online sources without manual copying 3. Knowledge Collections (Comprehensive) For more permanent knowledge bases. How to do it: Navigate to the \u2018Knowledge\u2019 tab in the workspace area Create a new collection (e.g., \u201cCompany Docs\u201d) Upload multiple documents to this collection Access this knowledge in chats with the # symbol: Type #Company Docs what is our refund policy TODO: add pic Best for : Building permanent knowledge bases from multiple documents 4. Dedicated Knowledge-Enabled Models The most powerful approach . How to do it: Create a new model in Open WebUI In the model settings, associate it with specific knowledge collections Add a custom system prompt and adjust model parameters (more on this later) Use this model for all questions related to that knowledge domain Best for : Creating specialized assistants for specific knowledge domains. Optimizing Your RAG System Let\u2019s make your RAG system faster and more accurate by tweaking some settings: Go to \u2018Settings > Admin Settings > Documents\u2019 TODO: add pic Here are the key settings to consider: Text Splitter Open WebUI offers two methods for splitting (or chunking) the files: Character Chunking : Splits text based on character count. Token Chunking (Recommended) : Splits text based on token count. Why token chunking is better: More reliable size estimation since LLMs process tokens rather than characters. It is easier to manage context window usage efficiently. Choosing the Right Chunk Size: The default chunk size is 500 tokens , which works well for most applications. But depending on your knowledge base, you might need to adjust it: Smaller chunks (~300 tokens or less) \u2192 Useful for particular queries where precision is key, such as legal documents or technical manuals. Larger chunks (~800\u20131000 tokens) \u2192 Preferred for structured data like research papers, where full paragraphs or sections provide better context. For comparison, 500 tokens are roughly 375 words or about two paragraphs of text , though this can vary based on content density and AI model. About Chunk Overlap : This setting allows consecutive chunks to share some content, preventing loss of meaning when text is split. A good starting point is 10\u201320% overlap to ensure smooth continuity. Embedding Models Embedding models convert text into numerical representations that help find similar content. In Open WebUI, you can select different embedding models: Pro tip : For RAG, using a small, fast model for embeddings is critical since it will process large amounts of text. The Snowflake/snowflake-arctic-embed-l-v2.0 model offers excellent performance at only ~500M parameters. Recommendation : Use the HuggingFace MTEB leaderboard to find fast and performant embedding models Chunk Size and Model Limitations : Most embedding models have a fixed token capacity (e.g., 512, 1024, or 8192 tokens). When setting the chunk size, make sure it does not exceed the embedding model\u2019s maximum token limit. TODO: add pic Use the MTEB leaderboard to find the best retrieval and reranking models. Hybrid Search with Re-ranking Highly Recommended: Enable hybrid search with CrossEncoder re-ranking to improve the relevance of retrieved documents. How re-ranking works in the RAG pipeline: Initial retrieval finds potentially relevant chunks based on vector similarity Re-ranking then evaluates these candidates more thoroughly The most relevant chunks (after re-ranking) are sent to the LLM This two-stage approach significantly improves the quality of retrieved information by filtering out false positives from the initial retrieval stage. Top K: Number of Retrieved Documents The Top K setting controls how many document chunks are retrieved for each query. This directly impacts the quality of responses by determining how much context is provided to the model. A greater value will help answer complex user queries that discuss multiple concepts. Pro Tip : The chunk size and number of retrieved documents matter! The formula (chunk_token_size \u00d7 num_retrieved_documents) should not exceed half of your LLM model\u2019s context window to leave room for the actual conversation. Relevance Threshold Adjust this to control how strict the system is when determining if a document section is relevant to a query. Lower values retrieve more (potentially less relevant) content; higher values are more selective. A threshold between 0.1 and 0.2 works well for most use cases, balancing recall and precision. Cloud Storage Integration Connect Google Drive or OneDrive to expand your knowledge sources (great for teams already using these platforms). TODO: add pic My RAG settings Data Processing for Knowledge Base Creation Before uploading documents to your RAG system, performing comprehensive data processing is crucial. This step significantly improves the quality and effectiveness of your knowledge base. Here are key strategies for preparing your data: Data Cleaning and Preparation Techniques Web Scraping Optimization When collecting data from the web, focus on removing unnecessary elements: Strip out navigation menus, headers, footers, and sidebars Remove duplicate text blocks Clean up formatting inconsistencies Content Normalization Replace images with descriptive alt text or image descriptions Standardize text formatting (consistent headings, paragraphs) Document Granularity: Break down large documents into focused, atomic units: Each document should answer one specific user question Create smaller, more targeted documents that are easier to retrieve Content Enhancement Summarize complex passages to improve readability Rewrite content to improve clarity and conciseness Add metadata to each document (e.g., source URL, timestamp of retrieval, relevant tags or categories, author, etc.) AI-Powered Curation: Leverage AI models for advanced data processing: Use a performant general-purpose AI to review and refine document collections Verify information accuracy Detect and remove redundant or low-quality content Suggest additional metadata or tags Recommended Storage Format Consider using a knowledge management system like Obsidian, which offers: Mind-map style note organization Single-topic focus for each document Built-in tagging system Easy lookup of related concepts Pro Tip: Refer to my GitHub repository for a comprehensive document-processing AI pipeline that automates these steps. Choosing the Right Model for RAG Not all models are created equal when answering RAG user queries. Key Considerations: Context Window: Larger is better for accommodating retrieved text and conversation. Model Size: Surprisingly, smaller models often work well with RAG since the retrieval system does the heavy lifting. Recommendations: For general RAG: Models with 7B to 14B parameters often have adequate capabilities to utilize the retrieved information and answer user queries. For complex reasoning with RAG: Consider larger models (30B+) but be aware of performance impacts Always prioritize models with longer context windows when doing RAG. Creating the Perfect System Prompt for RAG The system prompt is where you tell your AI how to use the retrieved information. Here\u2019s an example prompt for a documentation assistant: You are a specialized assistant with deep knowledge of Open WebUI, an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. Response Guidelines: Answer questions EXCLUSIVELY using the Open WebUI knowledge base provided to you Always cite your sources with BOTH the article title and reference URL When information is not found in the provided documents, clearly state this limitation DO NOT rely on general knowledge about AI platforms or related topics when documentation is unavailable Format all citations at the end of your response as: < Article Title , Article Title >> Interaction Approach: Seek clarification when user intent is ambiguous rather than making assumptions Ask targeted follow-up questions to understand user needs and goals better Maintain a helpful, knowledgeable tone focused on practical solutions When appropriate, suggest related features or documentation that might benefit the user Remember that your answers should prioritize accuracy over completeness\u2014it's better to acknowledge limitations than provide potentially incorrect information that is not supported by documentation. Key Elements of a Good RAG System Prompt: Clear instructions on when and how to use the retrieved information Guidelines for citing sources Explicit instructions for handling cases where information isn\u2019t found Tone and interaction style guidance If the model uses RAG as a tool-calling function, instruct it to request clarifications when encountering ambiguous queries before executing RAG. This ensures that the retrieved documents accurately address the user\u2019s query. Real-World RAG Examples Now, let\u2019s see RAG in action with two practical examples. All data and code used in these examples can be found in the linked GitHub repositories. Example 1: Wikipedia Knowledge Base Objective: Validate the RAG system\u2019s ability to retrieve precise information from a massive document collection. Dataset Overview: Source: WikiText-2 dataset Size: Over 40,000 Wikipedia articles Challenge: Find specific, relevant information within a vast knowledge base Setup Process: Download the WikiText-2 dataset Upload the entire dataset to Open WebUI as a knowledge collection. (Note: This process may take ~15 minutes, depending on your hardware) The Test: Query: \u201cCan you explain the evolution of Arundel House from a private mansion to a meeting place for the Royal Society?\u201d Without RAG: The model provides a general answer based on its training Without RAG, the model hallucinates information regarding English history. With RAG: The system retrieves the specific Wikipedia section about Arundel House and offers precise details with citations With RAG, the model retrieves factual information about Arundel House and provides citations. The result is nothing short of remarkable! Our carefully configured RAG system found the passage that directly addresses the historical transition of Arundel House in a corpus of over 40,000 Wikipedia articles. This isn\u2019t just search; it\u2019s intelligent information retrieval. Our RAG configuration \u2014 with its token-based chunking, relevance threshold, and hybrid search \u2014 perfectly worked to: Process and save the knowledge base Understand the semantics of the query Search through the extensive document collections Extract the most relevant information Present a targeted, citation-backed response Your turn: Navigate to a random paragraph in the WikiText datasource and ask a question related to that paragraph. See if the model can correctly find the excerpt you are interested in. Example 2: Open WebUI Documentation Assistant Objective: Create a specialized AI assistant to navigate Open WebUI documentation efficiently. Setup Process Document Collection: Scrape the official Open WebUI documentation and post-process documents using an AI pipeline. The code used to scrape the documentation can be found here Code for processing the document here Final data can be downloaded from here Create a new knowledge collection with these documents. Create a custom model with access to this knowledge collection. We adjust some model hyper-parameters to enhance the answer\u2019s clarity: Lower temperature for factual responses. Adjust to 0.1 Extend the context window. The default Open WebUI context length, 2048, may not hold all retrieved documents. Adjust it to a greater value while considering your model\u2019s context limit. (Note: a larger context will require more memory. If text generation slows down, it could mean Ollama is exceeding your GPU resources and trying to rebuild the model. In this case, reduce the context length to maintain consistent text generation speed and GPU memory usage) If your GPU memory usage looks something like this, then you probably need to lower the context length :p Custom system prompt for documentation navigation You are a specialized assistant with deep knowledge of Open WebUI, an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. Response Guidelines: Answer questions EXCLUSIVELY using the Open WebUI knowledge base provided to you Always cite your sources with BOTH the article title and reference URL When information is not found in the provided documents, clearly state this limitation DO NOT rely on general knowledge about AI platforms or related topics when documentation is unavailable Format all citations at the end of your response as: < Article Title , Article Title >> Interaction Approach: Seek clarification when user intent is ambiguous rather than making assumptions Ask targeted follow-up questions to understand user needs and goals better Maintain a helpful, knowledgeable tone focused on practical solutions When appropriate, suggest related features or documentation that might benefit the user Primary Goals: Help users effectively set up, configure, and utilize Open WebUI Troubleshoot common issues using only documented solutions Guide users to make the most of Open WebUI's offline capabilities Provide accurate technical information with proper attribution Remember that your answers should prioritize accuracy over completeness\u2014it's better to acknowledge limitations than provide potentially incorrect information that is not supported by documentation. Test RAG capabilities. Preview of the upcoming article in the series \ud83d\udc40 \ud83d\udc40 The results speak for themselves! The Documentation Assistant transforms complex technical documentation into a flexible and intelligent resource. This customized RAG setup empowers users by providing precise, citation-backed insights that convert intricate information into clear, actionable guidance. Whether you\u2019re troubleshooting, training, or improving internal processes, Open WebUI provides advanced RAG capabilities at your fingertips, ready to enhance your expertise and efficiency in your tasks. Pro Tip: This approach can easily be adapted for company internal documentation, technical manuals, or customer support knowledge bases. Conclusion By implementing RAG with Open WebUI, you\u2019ve transformed your local AI from a general chatbot into a specialized assistant with access to your private knowledge. This setup offers several advantages: Complete privacy: Your documents and queries never leave your machine Customization: The AI is now tailored to your specific knowledge domain Accuracy: Dramatically reduced hallucinations when answering factual questions Up-to-date information: Your AI is only as current as your knowledge base Most importantly, you\u2019ve built this entire system using free, open-source tools running on your hardware. Next Steps Ready to take your local AI to the next level? Experiment with different knowledge collections for different domains Try different models to find the best performance/accuracy balance Share your RAG setup with colleagues to create a collaborative knowledge system What knowledge bases are you planning to create? Let me know in the comments! Beyond RAG: Function Calling and Tools While RAG is powerful, it\u2019s just the beginning. In our following articles, we\u2019ll explore: Integrating custom Python functions with your AI Building tools that connect to external APIs and services Creating multi-agent systems that combine different models for complex tasks References Open WebUI: https://github.com/open-webui/open-webui Ollama: https://github.com/ollama/ollama WikiText2 data: https://github.com/pahautelman/wikitext2-raw Scraper for Open WebUI documentation: https://github.com/pahautelman/Open-WebUI-documentation-scraper Results of running the Obsidian AI pipeline on Open WebUI documentation: https://github.com/pahautelman/Obsidian-Text-Transformer-LocalAI-output AI pipeline for processing documents into Obsidian format: https://github.com/pahautelman/Obsidian-Text-Transformer-LocalAI (forked from https://github.com/Amine-LG/Obsidian-Text-Transformer-LocalAI)","title":"Supercharge Your Local AI with RAG and Custom Knowledge Bases"},{"location":"tutorials/open-webui-rag/#open-webui-tutorial-supercharge-your-local-ai-with-rag-and-custom-knowledge-bases","text":"In this second part of our local AI series, we\u2019ll dive into Retrieval-Augmented Generation (RAG) , showing you how to dramatically improve your AI\u2019s capabilities by connecting it to your own documents and knowledge sources. By the end, you\u2019ll have an AI that can answer questions based on private data without sending anything to external servers. TODO: add pic","title":"Open WebUI tutorial. Supercharge Your Local AI with RAG and Custom Knowledge Bases"},{"location":"tutorials/open-webui-rag/#table-of-contents","text":"Introduction What is RAG and Why Should You Care? RAG Options in Open WebUI Optimizing Your RAG System Data Processing for Knowledge Base Creation Choosing the Right Model for RAG Creating the Perfect System Prompt for RAG Real-World RAG Examples Wikipedia Knowledge Base Open WebUI Documentation Assistant Conclusion","title":"Table of Contents:"},{"location":"tutorials/open-webui-rag/#introduction","text":"In Part One, we set up a robust local AI environment with Ollama and Open WebUI, giving you a completely private, subscription-free alternative to commercial AI services. Today, we\u2019re taking this setup to the next level. While large language models (LLMs) are impressive, they come with two significant limitations: Their knowledge has a cutoff date (they don\u2019t know about recent events) They don\u2019t know about your private information (documents, data, etc.) This is where RAG comes in \u2014 a technique that allows your local AI to access, understand, and leverage custom knowledge sources to provide more accurate, personalized, and up-to-date responses. What you\u2019ll learn in this guide: Understanding how RAG works and why it matters How to set up and optimize RAG in Open WebUI. Real-world examples of RAG with practical applications. General tips and tricks for achieving the best results with your Open WebUI application. Let\u2019s transform your local AI from a general-purpose assistant into a specialized knowledge worker tailored to your specific needs.","title":"Introduction"},{"location":"tutorials/open-webui-rag/#what-is-rag-and-why-should-you-care","text":"TODO: add pic Retrieval-Augmented Generation visualized RAG, or Retrieval-Augmented Generation, might sound technical. Still, the concept is straightforward: when your AI receives a question, it first searches your documents for relevant information. Then, it uses the retrieved data to generate a more informed response. Here\u2019s the magic of RAG in simple terms: Your question \u2192 \u201cCan you explain how our company\u2019s vacation policy works?\u201d The RAG system \u2192 Searches your company handbook for relevant sections Retrieved context \u2192 Found policy details on pages 24\u201325 The AI \u2192 Uses this retrieved information to answer accurately Without RAG, your AI would either make something up (hallucinate) or say it doesn\u2019t know. RAG can provide precise answers based on your actual documents. Pro Tip : RAG is particularly valuable for specialized knowledge work \u2014 think customer support, research assistance, or mining insights from technical documentation.","title":"What is RAG and Why Should You Care?"},{"location":"tutorials/open-webui-rag/#under-the-hood-vectors-and-embeddings","text":"TODO: add pic At a technical level, RAG converts your documents into \u201cvectors\u201d \u2014 essentially mathematical representations of text that capture semantic meaning. When you upload documents to Open WebUI: The text is split into manageable chunks Each chunk is encoded into a vector using an embedding model These vectors are stored in a database optimized for similarity search When you ask a question, it\u2019s also converted to a vector The system finds the most similar document vectors to your question vector This vector-based approach allows the system to retrieve information based on meaning, not just keyword matching.","title":"Under the Hood: Vectors and Embeddings"},{"location":"tutorials/open-webui-rag/#rag-options-in-open-webui","text":"Open WebUI offers several ways to implement RAG without writing a single line of code. Let\u2019s explore each approach:","title":"RAG Options in Open WebUI"},{"location":"tutorials/open-webui-rag/#1-single-document-upload-quick-simple","text":"The most straightforward approach is uploading a document directly to the chat interface. How to do it: Start a new chat in Open WebUI Click the upload button in the chat input area Select your document Ask questions related to your document TODO: add pic Best for: Quick, one-off questions about a specific document.","title":"1. Single Document Upload (Quick &amp; Simple)"},{"location":"tutorials/open-webui-rag/#2-web-information-retrieval","text":"Need information from a specific webpage? How to do it: In your chat, type # followed by a URL Open WebUI fetches and parses the webpage content Ask questions about the content Example : #https://yourcompany.com/blog/latest-product what features were announced? Best for : Getting information from specific online sources without manual copying","title":"2. Web Information Retrieval"},{"location":"tutorials/open-webui-rag/#3-knowledge-collections-comprehensive","text":"For more permanent knowledge bases. How to do it: Navigate to the \u2018Knowledge\u2019 tab in the workspace area Create a new collection (e.g., \u201cCompany Docs\u201d) Upload multiple documents to this collection Access this knowledge in chats with the # symbol: Type #Company Docs what is our refund policy TODO: add pic Best for : Building permanent knowledge bases from multiple documents","title":"3. Knowledge Collections (Comprehensive)"},{"location":"tutorials/open-webui-rag/#4-dedicated-knowledge-enabled-models","text":"The most powerful approach . How to do it: Create a new model in Open WebUI In the model settings, associate it with specific knowledge collections Add a custom system prompt and adjust model parameters (more on this later) Use this model for all questions related to that knowledge domain Best for : Creating specialized assistants for specific knowledge domains.","title":"4. Dedicated Knowledge-Enabled Models"},{"location":"tutorials/open-webui-rag/#optimizing-your-rag-system","text":"Let\u2019s make your RAG system faster and more accurate by tweaking some settings: Go to \u2018Settings > Admin Settings > Documents\u2019 TODO: add pic Here are the key settings to consider:","title":"Optimizing Your RAG System"},{"location":"tutorials/open-webui-rag/#text-splitter","text":"Open WebUI offers two methods for splitting (or chunking) the files: Character Chunking : Splits text based on character count. Token Chunking (Recommended) : Splits text based on token count. Why token chunking is better: More reliable size estimation since LLMs process tokens rather than characters. It is easier to manage context window usage efficiently. Choosing the Right Chunk Size: The default chunk size is 500 tokens , which works well for most applications. But depending on your knowledge base, you might need to adjust it: Smaller chunks (~300 tokens or less) \u2192 Useful for particular queries where precision is key, such as legal documents or technical manuals. Larger chunks (~800\u20131000 tokens) \u2192 Preferred for structured data like research papers, where full paragraphs or sections provide better context. For comparison, 500 tokens are roughly 375 words or about two paragraphs of text , though this can vary based on content density and AI model. About Chunk Overlap : This setting allows consecutive chunks to share some content, preventing loss of meaning when text is split. A good starting point is 10\u201320% overlap to ensure smooth continuity.","title":"Text Splitter"},{"location":"tutorials/open-webui-rag/#embedding-models","text":"Embedding models convert text into numerical representations that help find similar content. In Open WebUI, you can select different embedding models: Pro tip : For RAG, using a small, fast model for embeddings is critical since it will process large amounts of text. The Snowflake/snowflake-arctic-embed-l-v2.0 model offers excellent performance at only ~500M parameters. Recommendation : Use the HuggingFace MTEB leaderboard to find fast and performant embedding models Chunk Size and Model Limitations : Most embedding models have a fixed token capacity (e.g., 512, 1024, or 8192 tokens). When setting the chunk size, make sure it does not exceed the embedding model\u2019s maximum token limit. TODO: add pic Use the MTEB leaderboard to find the best retrieval and reranking models.","title":"Embedding Models"},{"location":"tutorials/open-webui-rag/#hybrid-search-with-re-ranking","text":"Highly Recommended: Enable hybrid search with CrossEncoder re-ranking to improve the relevance of retrieved documents. How re-ranking works in the RAG pipeline: Initial retrieval finds potentially relevant chunks based on vector similarity Re-ranking then evaluates these candidates more thoroughly The most relevant chunks (after re-ranking) are sent to the LLM This two-stage approach significantly improves the quality of retrieved information by filtering out false positives from the initial retrieval stage.","title":"Hybrid Search with Re-ranking"},{"location":"tutorials/open-webui-rag/#top-k-number-of-retrieved-documents","text":"The Top K setting controls how many document chunks are retrieved for each query. This directly impacts the quality of responses by determining how much context is provided to the model. A greater value will help answer complex user queries that discuss multiple concepts. Pro Tip : The chunk size and number of retrieved documents matter! The formula (chunk_token_size \u00d7 num_retrieved_documents) should not exceed half of your LLM model\u2019s context window to leave room for the actual conversation.","title":"Top K: Number of Retrieved Documents"},{"location":"tutorials/open-webui-rag/#relevance-threshold","text":"Adjust this to control how strict the system is when determining if a document section is relevant to a query. Lower values retrieve more (potentially less relevant) content; higher values are more selective. A threshold between 0.1 and 0.2 works well for most use cases, balancing recall and precision.","title":"Relevance Threshold"},{"location":"tutorials/open-webui-rag/#cloud-storage-integration","text":"Connect Google Drive or OneDrive to expand your knowledge sources (great for teams already using these platforms). TODO: add pic My RAG settings","title":"Cloud Storage Integration"},{"location":"tutorials/open-webui-rag/#data-processing-for-knowledge-base-creation","text":"Before uploading documents to your RAG system, performing comprehensive data processing is crucial. This step significantly improves the quality and effectiveness of your knowledge base. Here are key strategies for preparing your data:","title":"Data Processing for Knowledge Base Creation"},{"location":"tutorials/open-webui-rag/#data-cleaning-and-preparation-techniques","text":"Web Scraping Optimization When collecting data from the web, focus on removing unnecessary elements: Strip out navigation menus, headers, footers, and sidebars Remove duplicate text blocks Clean up formatting inconsistencies Content Normalization Replace images with descriptive alt text or image descriptions Standardize text formatting (consistent headings, paragraphs) Document Granularity: Break down large documents into focused, atomic units: Each document should answer one specific user question Create smaller, more targeted documents that are easier to retrieve Content Enhancement Summarize complex passages to improve readability Rewrite content to improve clarity and conciseness Add metadata to each document (e.g., source URL, timestamp of retrieval, relevant tags or categories, author, etc.) AI-Powered Curation: Leverage AI models for advanced data processing: Use a performant general-purpose AI to review and refine document collections Verify information accuracy Detect and remove redundant or low-quality content Suggest additional metadata or tags Recommended Storage Format Consider using a knowledge management system like Obsidian, which offers: Mind-map style note organization Single-topic focus for each document Built-in tagging system Easy lookup of related concepts Pro Tip: Refer to my GitHub repository for a comprehensive document-processing AI pipeline that automates these steps.","title":"Data Cleaning and Preparation Techniques"},{"location":"tutorials/open-webui-rag/#choosing-the-right-model-for-rag","text":"Not all models are created equal when answering RAG user queries. Key Considerations: Context Window: Larger is better for accommodating retrieved text and conversation. Model Size: Surprisingly, smaller models often work well with RAG since the retrieval system does the heavy lifting. Recommendations: For general RAG: Models with 7B to 14B parameters often have adequate capabilities to utilize the retrieved information and answer user queries. For complex reasoning with RAG: Consider larger models (30B+) but be aware of performance impacts Always prioritize models with longer context windows when doing RAG.","title":"Choosing the Right Model for RAG"},{"location":"tutorials/open-webui-rag/#creating-the-perfect-system-prompt-for-rag","text":"The system prompt is where you tell your AI how to use the retrieved information. Here\u2019s an example prompt for a documentation assistant: You are a specialized assistant with deep knowledge of Open WebUI, an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.","title":"Creating the Perfect System Prompt for RAG"},{"location":"tutorials/open-webui-rag/#response-guidelines","text":"Answer questions EXCLUSIVELY using the Open WebUI knowledge base provided to you Always cite your sources with BOTH the article title and reference URL When information is not found in the provided documents, clearly state this limitation DO NOT rely on general knowledge about AI platforms or related topics when documentation is unavailable Format all citations at the end of your response as: < Article Title , Article Title >>","title":"Response Guidelines:"},{"location":"tutorials/open-webui-rag/#interaction-approach","text":"Seek clarification when user intent is ambiguous rather than making assumptions Ask targeted follow-up questions to understand user needs and goals better Maintain a helpful, knowledgeable tone focused on practical solutions When appropriate, suggest related features or documentation that might benefit the user Remember that your answers should prioritize accuracy over completeness\u2014it's better to acknowledge limitations than provide potentially incorrect information that is not supported by documentation. Key Elements of a Good RAG System Prompt: Clear instructions on when and how to use the retrieved information Guidelines for citing sources Explicit instructions for handling cases where information isn\u2019t found Tone and interaction style guidance If the model uses RAG as a tool-calling function, instruct it to request clarifications when encountering ambiguous queries before executing RAG. This ensures that the retrieved documents accurately address the user\u2019s query.","title":"Interaction Approach:"},{"location":"tutorials/open-webui-rag/#real-world-rag-examples","text":"Now, let\u2019s see RAG in action with two practical examples. All data and code used in these examples can be found in the linked GitHub repositories. Example 1: Wikipedia Knowledge Base Objective: Validate the RAG system\u2019s ability to retrieve precise information from a massive document collection. Dataset Overview: Source: WikiText-2 dataset Size: Over 40,000 Wikipedia articles Challenge: Find specific, relevant information within a vast knowledge base Setup Process: Download the WikiText-2 dataset Upload the entire dataset to Open WebUI as a knowledge collection. (Note: This process may take ~15 minutes, depending on your hardware) The Test: Query: \u201cCan you explain the evolution of Arundel House from a private mansion to a meeting place for the Royal Society?\u201d Without RAG: The model provides a general answer based on its training Without RAG, the model hallucinates information regarding English history. With RAG: The system retrieves the specific Wikipedia section about Arundel House and offers precise details with citations With RAG, the model retrieves factual information about Arundel House and provides citations. The result is nothing short of remarkable! Our carefully configured RAG system found the passage that directly addresses the historical transition of Arundel House in a corpus of over 40,000 Wikipedia articles. This isn\u2019t just search; it\u2019s intelligent information retrieval. Our RAG configuration \u2014 with its token-based chunking, relevance threshold, and hybrid search \u2014 perfectly worked to: Process and save the knowledge base Understand the semantics of the query Search through the extensive document collections Extract the most relevant information Present a targeted, citation-backed response Your turn: Navigate to a random paragraph in the WikiText datasource and ask a question related to that paragraph. See if the model can correctly find the excerpt you are interested in. Example 2: Open WebUI Documentation Assistant Objective: Create a specialized AI assistant to navigate Open WebUI documentation efficiently. Setup Process Document Collection: Scrape the official Open WebUI documentation and post-process documents using an AI pipeline. The code used to scrape the documentation can be found here Code for processing the document here Final data can be downloaded from here Create a new knowledge collection with these documents. Create a custom model with access to this knowledge collection. We adjust some model hyper-parameters to enhance the answer\u2019s clarity: Lower temperature for factual responses. Adjust to 0.1 Extend the context window. The default Open WebUI context length, 2048, may not hold all retrieved documents. Adjust it to a greater value while considering your model\u2019s context limit. (Note: a larger context will require more memory. If text generation slows down, it could mean Ollama is exceeding your GPU resources and trying to rebuild the model. In this case, reduce the context length to maintain consistent text generation speed and GPU memory usage) If your GPU memory usage looks something like this, then you probably need to lower the context length :p Custom system prompt for documentation navigation You are a specialized assistant with deep knowledge of Open WebUI, an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.","title":"Real-World RAG Examples"},{"location":"tutorials/open-webui-rag/#response-guidelines_1","text":"Answer questions EXCLUSIVELY using the Open WebUI knowledge base provided to you Always cite your sources with BOTH the article title and reference URL When information is not found in the provided documents, clearly state this limitation DO NOT rely on general knowledge about AI platforms or related topics when documentation is unavailable Format all citations at the end of your response as: < Article Title , Article Title >>","title":"Response Guidelines:"},{"location":"tutorials/open-webui-rag/#interaction-approach_1","text":"Seek clarification when user intent is ambiguous rather than making assumptions Ask targeted follow-up questions to understand user needs and goals better Maintain a helpful, knowledgeable tone focused on practical solutions When appropriate, suggest related features or documentation that might benefit the user","title":"Interaction Approach:"},{"location":"tutorials/open-webui-rag/#primary-goals","text":"Help users effectively set up, configure, and utilize Open WebUI Troubleshoot common issues using only documented solutions Guide users to make the most of Open WebUI's offline capabilities Provide accurate technical information with proper attribution Remember that your answers should prioritize accuracy over completeness\u2014it's better to acknowledge limitations than provide potentially incorrect information that is not supported by documentation. Test RAG capabilities. Preview of the upcoming article in the series \ud83d\udc40 \ud83d\udc40 The results speak for themselves! The Documentation Assistant transforms complex technical documentation into a flexible and intelligent resource. This customized RAG setup empowers users by providing precise, citation-backed insights that convert intricate information into clear, actionable guidance. Whether you\u2019re troubleshooting, training, or improving internal processes, Open WebUI provides advanced RAG capabilities at your fingertips, ready to enhance your expertise and efficiency in your tasks. Pro Tip: This approach can easily be adapted for company internal documentation, technical manuals, or customer support knowledge bases.","title":"Primary Goals:"},{"location":"tutorials/open-webui-rag/#conclusion","text":"By implementing RAG with Open WebUI, you\u2019ve transformed your local AI from a general chatbot into a specialized assistant with access to your private knowledge. This setup offers several advantages: Complete privacy: Your documents and queries never leave your machine Customization: The AI is now tailored to your specific knowledge domain Accuracy: Dramatically reduced hallucinations when answering factual questions Up-to-date information: Your AI is only as current as your knowledge base Most importantly, you\u2019ve built this entire system using free, open-source tools running on your hardware. Next Steps Ready to take your local AI to the next level? Experiment with different knowledge collections for different domains Try different models to find the best performance/accuracy balance Share your RAG setup with colleagues to create a collaborative knowledge system What knowledge bases are you planning to create? Let me know in the comments! Beyond RAG: Function Calling and Tools While RAG is powerful, it\u2019s just the beginning. In our following articles, we\u2019ll explore: Integrating custom Python functions with your AI Building tools that connect to external APIs and services Creating multi-agent systems that combine different models for complex tasks References Open WebUI: https://github.com/open-webui/open-webui Ollama: https://github.com/ollama/ollama WikiText2 data: https://github.com/pahautelman/wikitext2-raw Scraper for Open WebUI documentation: https://github.com/pahautelman/Open-WebUI-documentation-scraper Results of running the Obsidian AI pipeline on Open WebUI documentation: https://github.com/pahautelman/Obsidian-Text-Transformer-LocalAI-output AI pipeline for processing documents into Obsidian format: https://github.com/pahautelman/Obsidian-Text-Transformer-LocalAI (forked from https://github.com/Amine-LG/Obsidian-Text-Transformer-LocalAI)","title":"Conclusion"}]}